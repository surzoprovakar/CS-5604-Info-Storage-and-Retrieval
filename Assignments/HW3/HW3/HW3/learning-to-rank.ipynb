{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "\n",
    "In this assignment, we will be creating ranking functions (Part 1) and investigating poitwise and pairwise learning-to-rank approaches (Part 2). \n",
    "\n",
    "The HW is due **Friday, December 3, 2021 @ 11:59 pm**. You can form teams of three students, two students, or work individually. Note that there exists no difference in terms of grading, i.e., we will grade the same for one and two person teams. **Only one of the team members needs to submit the HW.**\n",
    "\n",
    "Please submit a zip file containing the two parts of the assignment:\n",
    "\n",
    "1. **Ranking Code (47%)**: Make sure to finish the ranking.ipynb Jupyter notebook first.\n",
    "2. **LTR Code (53%)**: In the learning-to-rank.ipynb notebook, we will implement LTR models. Submit both Jupyter notebooks.\n",
    "\n",
    "\n",
    "### HW3 - Learning to Rank (53% of total HW3 grade)\n",
    "\n",
    "In the first part of this assignment, we examined various ways of ranking documents given a query; however, weights for different features were not learned automatically but set manually. As more and more ranking signals are investigated, integrating more features becomes challenging as it would be hard to come up with a single ranking function like BM25 for arbitrary features. \n",
    "\n",
    "In this assignment, you will be investigating different approaches to the learning to rank task that you have learned: (1) the pointwise approach using linear regression and (2) the pairwise approach employing gradient boosted decision trees. The goal is to let these algorithms learn weights automatically for various features. \n",
    "\n",
    "More specifically, it involves the following tasks (weights are for the programming assignment as a whole):\n",
    "* [Task 1: Pointwise Approach and Linear Regression (10%)](#Task-1:-Pointwise-Approach-and-Linear-Regression-(10%)): Implement a pointwise approach with linear regression based on basic tf-idf features\n",
    "* [Task 2: Pairwise Approach and Gradient Boosted Decision Trees (15%)](#Task-2:-Pairwise-Approach-and-Gradient-Boosted-Decision-Trees-(15%)): Implement an instance of the pairwise approach with the help of gradient boosted decision trees, using basic tf-idf features\n",
    "* [Task 3: Train Your Best Model (20%)](#Task-3:-Adding-More-Features-(20%)) Train your best model, and experiment with more features such as BM25, Smallest Window, and PageRank\n",
    "* [Task 4: Report (8%)](#Task-4:-Report-(8%)): Write up a summary report and answer some questions about the above tasks\n",
    "\n",
    "\n",
    "__Grading__\n",
    "- Part of your grade will be based on your model's performance on a hidden test set. \n",
    "- You will get full credit for solutions that receive NDCG scores within reasonable range of the NDCG scores received by the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `base_classes` folder contains useful class definitions (not to be edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('base_classes')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add additional imports below as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add additional imports here\n",
    "\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset is the same as what you used in the first part of this HW.**\n",
    "\n",
    "As in the first part of the HW, we have partitioned the data into two sets for you: \n",
    "1. Training set (hw3.(signal|rel).train)\n",
    "2. Development set (hw3.(signal|rel).dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading previous code\n",
    "\n",
    "We load the AScorer class that you completed in the first part of HW3. \n",
    "\n",
    "**Note that you may need to make updates to this class for completing the tasks in this notebook.**\n",
    "\n",
    "We also load the Idf class that you can use to get document frequency values based on the corpus. You will also need to load the Rank class for the computation of NDCG scores on the tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_classes.load_train_data import load_train_data\n",
    "from base_classes.id_map import IdMap\n",
    "from base_classes.ndcg import NDCG\n",
    "from base_classes.query import Query\n",
    "from base_classes.document import Document\n",
    "from base_classes.ascore import AScorer\n",
    "from base_classes.build_idf import Idf\n",
    "from base_classes.rank import Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Pointwise Approach and Linear Regression (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ranking, each query $q_i$ will be associated with a set of documents, and for each document $j$, we extract a query-document feature vector $x_{i,j}$. There is also a label $y_{i,j}$ associated with each query-document vector $x_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pointwise approach, such group structure in ranking is ignored, and we simply view our training data as $\\{(x_{i}, y_{i})\\}$, where each instance consists of a query-document feature vector $x_{i}$ and a label $y_{i}$ (which is a relevance score as in the first part of this programming assignment). The ranking problem amounts to learning a function $f$ such that $f(x_{i})$ closely matches $y_{i}$.\n",
    "\n",
    "In this task, we consider a very simple instance of the pointwise approach, the *linear regression* approach. That is, we will use a linear function $f$ which gives a score to each query-document feature vector $x$ as follows: $f(x) = wx+b$. Here, the weight vector ${w}$ and the bias term $b$ are parameters that we need to learn to minimize the loss function as defined below:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m (f(x_{i})-y_{i})^2\n",
    "\\end{equation}\n",
    "This formulation is also referred to as the *ordinary least squares* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Designing Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each query-document pair as a five-dimensional vector of query vector-document vector (tf-idf) scores. Each dimension corresponds to a document field -- url, title, header, body, and anchor. Specifically, given a query vector $q$ and a document vector $d_{f}$ of a document field $f$, the tf-idf score is the dot product $q \\cdot d_{f}$. \n",
    "\n",
    "To start with, use query and document vectors with lnn.ltc weighting (as represented in SMART notation ddd.qqq). In other words, begin by using:\n",
    "\n",
    "1) For the document vectors, \"lnn\":\n",
    "    - logarithmic term frequency of query terms in documents\n",
    "    - no document frequency \n",
    "    - no normalization\n",
    "2) For the query vector, \"ltc\":\n",
    "    - logarithmic term frequency for words in query\n",
    "    - idf (inverse document frequency)\n",
    "    - cosine (i.e., L2) normalization\n",
    "    \n",
    "Then, experiment with a few weighting schemes other than lnn.ltc.  Refer to the figure below for other possible weighting schemes. You will report which weighting scheme yields the best performance in Task 4.\n",
    "\n",
    "<img src=\"fig/IIR_fig_6.15.png\">\n",
    "Figure is from Pg.128 http://nlp.stanford.edu/IR-book/pdf/06vect.pdf\n",
    "\n",
    "\n",
    "A few important notes:\n",
    "- Creating these vectors is similar to the exercise you performed in computing cosine similarity in the first part of this programming assignment\n",
    "- Make modifications to the AScorer class in order to try to implement other weighting mechanisms \n",
    "- **You will use these basic feature vectors for both Task 1 and Task 2. Do not use any other signals or features for Tasks 1 and 2; you will have the opportunity to use these features in Task 3.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features (signal_file, idf):\n",
    "    '''\n",
    "    Create a feature vector from the signal file and from the idf_dict. \n",
    "\n",
    "    Args:\n",
    "        signal_file: filepath to signal file\n",
    "        idf: object of class Idf (with idf built)\n",
    "\n",
    "    Returns:\n",
    "        feature_vec (numpy array of dimension (N, 5)): N is the number of (query, document)\n",
    "        pairs in the relevance file.\n",
    "    '''\n",
    "\n",
    "    # Experiment with different values of weighting below. Note that this uses dddqqq notation.\n",
    "    # Make sure to set weighting to the best value prior to submitting your code.\n",
    "    # You should be able to support lnn.ltc weighting, along with any other weighting that you experiment with\n",
    "\n",
    "\n",
    "    WEIGHTING = 'lnnltc' \n",
    "\n",
    "    assert len(WEIGHTING) == 6, \"Invalid weighting scheme.\"        \n",
    "\n",
    "    feature_vec = []\n",
    "\n",
    "    ### Begin your code\n",
    "    \n",
    "    zones = ['headers', 'anchor', 'url', 'body_hits', 'anchor']\n",
    "    \n",
    "    for i, doc_dict in load_train_data(signal_file).items():\n",
    "        terms = Counter(i.query_words)\n",
    "        query_vector = AScorer(idf).get_query_vector(i)\n",
    "        query_vector = [query_vector.get(k) for k in terms.keys()]\n",
    "        query_vector = np.array(query_vector)\n",
    "        query_vector /= np.linalg.norm(query_vector)\n",
    "        #print(query_vector)\n",
    "        \n",
    "        for j in doc_dict.values():\n",
    "            doc_vec = AScorer(idf).get_doc_vector(i, j)\n",
    "            #print(doc_vec)\n",
    "         \n",
    "            for t in terms:\n",
    "                zone = lambda k: np.sum(query_vector * np.array([doc_vec[k][t]])) if k in doc_vec else 0\n",
    "\n",
    "            doc_arr = [] \n",
    "            for z in zones:\n",
    "                doc_arr.append(zone(z))\n",
    "            feature_vec.append(doc_arr)\n",
    "            \n",
    "    feature_vec = np.array(feature_vec)\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_relevance (relevance_file):\n",
    "    '''\n",
    "    Extract relevance scores from the relevance file. This should be a simple wrapper (<10 lines) over\n",
    "    the get_rel_scores() function in the NDCG class.\n",
    "\n",
    "    Args:\n",
    "        relevance_file: filepath to relevance file\n",
    "\n",
    "    Returns:\n",
    "        relevance_vec (numpy array of dimension (N,)): N is the number of (query, document)\n",
    "        pairs in the relevance file.   \n",
    "        ndcg_obj: NDCG object which contains relevance scores\n",
    "    '''  \n",
    "\n",
    "\n",
    "    relevance_vec = []\n",
    "    ndcg_obj = NDCG()\n",
    "\n",
    "    ### Begin your code\n",
    "    \n",
    "    ndcg_obj.get_rel_scores(relevance_file)\n",
    "    for url in ndcg_obj.rel_scores.values():\n",
    "        relevance_vec += list(url.values())\n",
    "    relevance_vec = np.array(relevance_vec)\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return relevance_vec, ndcg_obj   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Training a Linear Regression Model\n",
    "\n",
    "Implement the PointwiseLearner class below. You may use the LinearRegression class from the sklearn package. If you use the LinearRegression class, set fit_intercept to true and normalize to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, x, y):\n",
    "    \n",
    "        '''\n",
    "        - Train your linear regression model using the LinearRegression class \n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of query, document pairs. \n",
    "                Is the independent variable for linear regression. \n",
    "\n",
    "                y (numpy array of dimension (N,)): Relevance score for each query, document pair. \n",
    "                Is the dependent variable for linear regresion.\n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        ### Begin your code\n",
    "        \n",
    "        self.model = LinearRegression().fit(x,y)\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, x):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of (query, document) pairs. \n",
    "                Predictions are made on this input feature array.\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained linear regression model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "        \n",
    "        return self.model.predict(x)\n",
    "\n",
    "        ### End your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Docs is 98998\n",
      "Total Number of Terms is 347071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm = PointwiseLearner()\n",
    "\n",
    "idf = Idf()\n",
    "\n",
    "#Get train features and relevance\n",
    "\n",
    "train_signal_file = \"data/hw3.signal.train\"\n",
    "train_rel_file = \"data/hw3.rel.train\"\n",
    "train_features = get_features(train_signal_file, idf)\n",
    "train_relevance, train_ndcg = get_relevance(train_rel_file)\n",
    "assert train_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])\n",
    "\n",
    "#Train linear regression model\n",
    "\n",
    "lm.train_model(train_features, train_relevance)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "dev_signal_file = \"data/hw3.signal.dev\"\n",
    "dev_rel_file = \"data/hw3.rel.dev\"\n",
    "dev_features = get_features(dev_signal_file, idf)\n",
    "dev_relevance, dev_ndcg =  get_relevance(dev_rel_file)\n",
    "dev_predicts = lm.predict_model(dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your code passes the sanity check below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dev_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using the predictions from your trained model, compute the mean squared error and NDCG score that you receive. \n",
    "\n",
    "Include the score you received in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_calc_for_LTR (dev_ndcg, dev_predicts, out_file=\"ranked_result_default\"):\n",
    "\n",
    "    ''' We provide this function to calculate the average NDCG score given a predicted score and a ground truth score.\n",
    "        Note that the code below calls rank_with_score() in the Rank class, so the correct value for NDCG \n",
    "        depends on the correct implmementation of that function.\n",
    "         Args:\n",
    "                dev_ndcg (type NDCG): Object that contains the \"ground truth\" relevance scores in dev_ndcg.rel_scores \n",
    "                dev_predicts: numpy array of dimension (N,) which contains predicted scores for a dataset.\n",
    "                out_file: filename to which the ranked_result_file is written\n",
    "            \n",
    "        Returns: avg_ndcg_score: Scalar that averages NDCG score across all queries. \n",
    "    \n",
    "    '''\n",
    "    idx = 0\n",
    "    dev_predicts_dict = {}\n",
    "\n",
    "    #Converts the dev_predicts vector into query->url->score dict\n",
    "    for query, url_dict in dev_ndcg.rel_scores.items():\n",
    "        query_obj = Query(query) #Converts str to Query object\n",
    "        dev_curr_dict = {}\n",
    "        for url in url_dict.keys():\n",
    "            dev_curr_dict[url] = dev_predicts[idx]\n",
    "            idx+=1\n",
    "        dev_predicts_dict[query_obj] = dev_curr_dict\n",
    "\n",
    "    #Orders dev_predicts_dict. This remains a Query->url->score dict after ordering.\n",
    "    #Note that this depends on your implementation of the rank_with_score() function in the Rank class.\n",
    "    r = Rank()\n",
    "    dev_predicts_dict_ordered = r.rank_with_score(dev_predicts_dict)\n",
    "\n",
    "    #Creates a Query->Document->score dict called dev_predicts_ranks that will be written to file.\n",
    "    dev_data = load_train_data(dev_signal_file) #Query->Document dict\n",
    "\n",
    "    dev_predicts_ranked = {} #The Query->Document->Score dict that will be written to file.\n",
    "    for query in dev_predicts_dict_ordered:\n",
    "        doc_to_score = {}\n",
    "        for url in dev_predicts_dict_ordered[query]:\n",
    "            doc = dev_data[query][url]\n",
    "            doc_to_score[doc] = dev_predicts_dict_ordered[query][url]\n",
    "        dev_predicts_ranked[query] = doc_to_score\n",
    "\n",
    "    #Writes dev_predicts_ranked to file.\n",
    "    if not os.path.exists(\"output\"): os.mkdir(\"output\")\n",
    "    ranked_result_file = os.path.join(\"output\", out_file)\n",
    "    r.write_ranking_to_file(dev_predicts_ranked, ranked_result_file)\n",
    "\n",
    "    #Uses the NDCG class to get the NDCG score\n",
    "    dev_ndcg.read_ranking_calc(ranked_result_file)\n",
    "    avg_ndcg_score = dev_ndcg.get_avg_ndcg()\n",
    "    return avg_ndcg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8517544626817459\n",
      "Write ranking result to output\\ranked_result_pointwise sucessfully!\n",
      "Average NDCG score: 0.7678861819391749\n"
     ]
    }
   ],
   "source": [
    "# Compute mean squared error and NDCG Score\n",
    "\n",
    "mse = mean_squared_error(dev_relevance, dev_predicts)\n",
    "\n",
    "print (\"Mean Squared Error:\", mse)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts, \"ranked_result_pointwise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Pairwise Approach and Gradient Boosted Decision Trees (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the LambdaMART algorithm to implement Gradient Boosted Decision Trees. \n",
    "\n",
    "LambdaMART is the boosted tree version of an earlier algorithm, LambdaRank. The full evolution of algorithms from RankNet through LambdaRANK, MART and LambdaMART is presented below (Page 16 and 17 are particularly important). \n",
    "https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf\n",
    "\n",
    "The relevant lecture notes are **CS5604LearningToRank.pdf** accessible at [Canvas Files](https://canvas.vt.edu/courses/136044/files?preview=20605783)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the XGBoost package to implement LambdaMART. You may find it helpful to read the documentation here: https://xgboost.readthedocs.io/en/latest/get_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter description (not exhaustive, see here for more details): https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "General Parameters (**make sure to use the following values**):\n",
    "- \"booster\": use \"gbtree\". Uses a tree-based model for boosting\n",
    "- \"objective\": use \"rank:pairwise\". Uses the LambdaMART algorithm to minimize pairwise loss. \n",
    "- \"eval_metric: use \"ndcg\" (while we will be evaluating your performance solely based on ndcg, feel free to test performance on other metrics)\n",
    "\n",
    "Hyperparamters to be tuned (not exhaustive):\n",
    "- \"eta\": Learning rate\n",
    "- \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "- \"max_depth\": Maximum depth of a tree\n",
    "- \"subsample\": Subsample ratio of training instances to prevent overfitting\n",
    "\n",
    "When training, you should also experiment with early stopping to prevent overfitting. Take a look at the description of early stopping here: https://xgboost.readthedocs.io/en/latest/python/python_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_dict = load_train_data(train_signal_file)\n",
    "train_groups = []\n",
    "for query, url_dict in train_query_dict.items():\n",
    "    train_groups.append(len(url_dict))\n",
    "    \n",
    "assert len(train_groups) == 700, 'Expected 700 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "dev_query_dict = load_train_data(dev_signal_file)\n",
    "dev_groups = []\n",
    "for query, url_dict in dev_query_dict.items():\n",
    "    dev_groups.append(len(url_dict))\n",
    "    \n",
    "assert len(dev_groups) == 100, 'Expected 100 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "dtrain = xgb.DMatrix(train_features, label = train_relevance)\n",
    "dtrain.set_group(train_groups)\n",
    "ddev = xgb.DMatrix(dev_features, label = dev_relevance) \n",
    "ddev.set_group(dev_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, dtrain, evallist):\n",
    "    \n",
    "        '''\n",
    "        - Specifies parameters for XGBoost training\n",
    "        - Trains model\n",
    "\n",
    "        Args:\n",
    "                dtrain (type DMatrix): DMatrix is a internal data structure that used by XGBoost \n",
    "                which is optimized for both memory efficiency and training speed.\n",
    "                \n",
    "                evallist (array of tuples): The datasets on which the algorithm reports performance as training takes place\n",
    "                \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        num_rounds = 48 #Experiment with different values of this parameter\n",
    "        \n",
    "        ### Begin your code\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params[\"booster\"] = \"gbtree\"\n",
    "        self.params[\"objective\"] = \"rank:pairwise\"\n",
    "        self.params[\"eval_metric\"] = \"ndcg\"\n",
    "        self.params[\"eta\"] = 1/(num_rounds*num_rounds)\n",
    "        self.params[\"gamma\"] = 1/(num_rounds*num_rounds)\n",
    "        self.params[\"max_depth\"] = num_rounds\n",
    "        self.params[\"subsample\"] = 1\n",
    "        \n",
    "        self.model = xgb.train(\n",
    "            self.params,\n",
    "            dtrain=dtrain,\n",
    "            evals=evallist,\n",
    "            early_stopping_rounds=num_rounds,\n",
    "            num_boost_round=num_rounds)\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, dtest):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                dtest (type DMatrix): DMatrix that contains the dev/test signal data\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained  model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "        \n",
    "        return self.model.predict(dtest, iteration_range=(0,self.model.best_iteration+1))\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg:0.86938\n",
      "[1]\ttrain-ndcg:0.88468\n",
      "[2]\ttrain-ndcg:0.89617\n",
      "[3]\ttrain-ndcg:0.89859\n",
      "[4]\ttrain-ndcg:0.89984\n",
      "[5]\ttrain-ndcg:0.90136\n",
      "[6]\ttrain-ndcg:0.90058\n",
      "[7]\ttrain-ndcg:0.90385\n",
      "[8]\ttrain-ndcg:0.90307\n",
      "[9]\ttrain-ndcg:0.90469\n",
      "[10]\ttrain-ndcg:0.90540\n",
      "[11]\ttrain-ndcg:0.90648\n",
      "[12]\ttrain-ndcg:0.90619\n",
      "[13]\ttrain-ndcg:0.90775\n",
      "[14]\ttrain-ndcg:0.90683\n",
      "[15]\ttrain-ndcg:0.90662\n",
      "[16]\ttrain-ndcg:0.90781\n",
      "[17]\ttrain-ndcg:0.90820\n",
      "[18]\ttrain-ndcg:0.90837\n",
      "[19]\ttrain-ndcg:0.90825\n",
      "[20]\ttrain-ndcg:0.90895\n",
      "[21]\ttrain-ndcg:0.90930\n",
      "[22]\ttrain-ndcg:0.91002\n",
      "[23]\ttrain-ndcg:0.90939\n",
      "[24]\ttrain-ndcg:0.90968\n",
      "[25]\ttrain-ndcg:0.90988\n",
      "[26]\ttrain-ndcg:0.90921\n",
      "[27]\ttrain-ndcg:0.90890\n",
      "[28]\ttrain-ndcg:0.90977\n",
      "[29]\ttrain-ndcg:0.90974\n",
      "[30]\ttrain-ndcg:0.91011\n",
      "[31]\ttrain-ndcg:0.91022\n",
      "[32]\ttrain-ndcg:0.91043\n",
      "[33]\ttrain-ndcg:0.91005\n",
      "[34]\ttrain-ndcg:0.91061\n",
      "[35]\ttrain-ndcg:0.91090\n",
      "[36]\ttrain-ndcg:0.91139\n",
      "[37]\ttrain-ndcg:0.91122\n",
      "[38]\ttrain-ndcg:0.91094\n",
      "[39]\ttrain-ndcg:0.91120\n",
      "[40]\ttrain-ndcg:0.91150\n",
      "[41]\ttrain-ndcg:0.91186\n",
      "[42]\ttrain-ndcg:0.91168\n",
      "[43]\ttrain-ndcg:0.91212\n",
      "[44]\ttrain-ndcg:0.91195\n",
      "[45]\ttrain-ndcg:0.91234\n",
      "[46]\ttrain-ndcg:0.91233\n",
      "[47]\ttrain-ndcg:0.91217\n"
     ]
    }
   ],
   "source": [
    "#Train a gradient boosted decision trees model.\n",
    "\n",
    "model = GBDTLearner()\n",
    "evallist = [(dtrain, 'train')]\n",
    "model.train_model(dtrain, evallist)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "\n",
    "dev_predicts_gbdt = model.predict_model(ddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write ranking result to output\\ranked_result_gbdt sucessfully!\n",
      "Average NDCG score: 0.7618741489108412\n"
     ]
    }
   ],
   "source": [
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_gbdt, \"ranked_result_gbdt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Adding More Features (20%)\n",
    "\n",
    "Putting it all together! In this part, train your best model - and feel free to use additional features! Experiment with the following to see which yields the best performance on the dev set:\n",
    "\n",
    "1. Using smallest window feature from the first part of this programming assignment\n",
    "2. Using Pagerank from the idf file\n",
    "\n",
    "In addition, you may also choose to experiment with using word vectors. We provide GLoVE embeddings for the words in our vocabulary, which you can download with the help of embedding.py in the base_classes folder. (Disclaimer: downloading might be slow)\n",
    "\n",
    "You may choose to write several helper functions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "    ### Begin your code\n",
    "    \n",
    "        self.model = PointwiseLearner()\n",
    "\n",
    "    ### End your code\n",
    "   \n",
    "    # You may choose to write other helper functions below \n",
    "    # (such as to augment feature array with additional features)\n",
    "    \n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    \n",
    "    \n",
    "    def train_and_predict(self, train_signal_file, train_rel_file, test_signal_file, idf):\n",
    "    \n",
    "        '''\n",
    "        - Receives the training signal and relevance files as parameters\n",
    "        - Creates a feature vector associated with the signal file\n",
    "        - Trains the best possible model on the training data\n",
    "        - Using the trained model, makes a prediction on the test_signal_file\n",
    "        \n",
    "        - \n",
    "\n",
    "        Args:\n",
    "            train_signal_file: filename of training signal\n",
    "            train_rel_file: filename of training relevance file\n",
    "            test_signal_file: filename containing dev/test signal\n",
    "            idf: object of class IDF, containing a fully built idf dictionary\n",
    "            \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        test_predictions = []\n",
    "    \n",
    "        ### Begin your code\n",
    "        \n",
    "        \n",
    "        train_features = get_features(train_signal_file, idf)\n",
    "        train_relevance, train_ndcg = get_relevance(train_rel_file)\n",
    "        \n",
    "        self.model.train_model(train_features, train_relevance)\n",
    "        test_features = get_features(test_signal_file, idf)\n",
    "        test_predictions = self.model.predict_model(test_features)\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Docs is 98998\n",
      "Total Number of Terms is 347071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write ranking result to output\\ranked_result_best sucessfully!\n",
      "Average NDCG score: 0.7678861819391749\n"
     ]
    }
   ],
   "source": [
    "model = BestModel()\n",
    "idf = Idf()\n",
    "train_signal_file = \"data/hw3.signal.train\"\n",
    "train_rel_file = \"data/hw3.rel.train\"\n",
    "dev_signal_file = \"data/hw3.signal.dev\"\n",
    "\n",
    "dev_predicts_best = model.train_and_predict(train_signal_file, train_rel_file, dev_signal_file, idf)\n",
    "\n",
    "dev_rel_file = \"data/hw3.rel.dev\"\n",
    "dev_relevance, dev_ndcg = get_relevance(dev_rel_file)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_best, \"ranked_result_best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Report (8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is meant to be relatively more open-ended as you describe the model choices you made in this assignment. **Please keep your report concise.** Be sure to document any design decisions you made, and provide a brief rationale for them. \n",
    "\n",
    "You may choose to insert cells below to generate tables or plots if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Design of feature vectors (Task 1 and 2) (1%)\n",
    "\n",
    "For each (query, document) pair, in designing your feature vector from query vector and document vectors, you had various possible options for (i) term frequency, (ii) document frequency and (iii) normalization. The default option we recommended you start with for the feature vector is lnn.ltc (using the SMART notation ddd.qqq).\n",
    "\n",
    "What other choices did you experiment with? How did the performance compare across these choices? What might be the rationale for this difference in performance across the various models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have used tf-idf as a feature vector for each query-document pair. Besides it, we experimented with raw term frequency. And from our experiment, we see that using tf-idf to design feature vector give better performance. Because while using raw term frequency, a term that has occurred 20 times doesn't mean that always the term carries twenty-time significance, there may be that less occurred term carries more significance. So just implementing a feature based on raw term frequency doesn't always guarantee good performance. But using tf-idf we have a basic metric to extract the most descriptive terms in a document and we can easily compute the similarity between two documents using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Hyperparameter tuning  (Task 2) (1%)\n",
    "\n",
    "Briefly describe the hyperparameters you tuned for your implementation of XGBoost. \n",
    "Which hyperparameters were most consequential to the performance of the model?\n",
    "\n",
    "Provide an intuition, based on your understanding of the LambdaMART algorithm, for why the performance of the model varied as it did with the hyperparameters you tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To implement XGBoost, the hyperparameters we have used are <br>\n",
    "\"eta\" : 4.3x10^-3<br> \"gamma\" : 4.3x10^-3<br> \"max_depth\" : 48<br> \"subsample\" : 48<br>\n",
    "We believe the hyperparameters **eta, gamma, subsample** were most consequential to the performance of the model as eta refers learning rate of the model, gamma is the loss reduction and this value is smaller and subsample refers to ratio of the instances to prevent overfitting and the value we used didn't cause overfitting for our model. We used K-fold method to finalize these hyper-parameters<br><br>\n",
    "From our understanding of the LambdaMart algorithm, for a given learning rate the Gradient of cross-entropy times pairwise change in target metric when the loss reduction is *1/(number of rounds)*. But we have used *1/(number of boost rounds*number of boost rounds)* and we think because of that the performance of our model varied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model Design and Ablation Analysis (Task 3) (3%)\n",
    "\n",
    "You had the option to include various additional features in your model design. Which features did you experiment with? Which features did you end up using in your final model, and why? \n",
    "\n",
    "We expect ablation analysis on which features provided useful signals and which ones did not. Please include at least 1-2 plots and/or tables for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I found Linear Regresssion as the best performing model out of the Two models implmented above which are Linear Regression and Gradient Boosting. So I performed IDF code part on the input train and test data and then processed with the Best Linear Regression model to process the output. But intrestingly I got the same NDCG score as the normal Linear Regression Model with implementing IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Error Analysis (Task 3) (3%)\n",
    "\n",
    "Analyze your errors for the best performing model you trained. Please include 1-2 plots and/or tables for this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Error Analysis can be done on 3 different levels - Predictions, Data and Features<br><br><b>1. Predictions : </b> In this we make predictions of why there is loss in training(Mis-Classification) and this can be done using a) Building Confusion Matrix and observing trend as to why some data is mis-classified, b) Comparing our model with Baseline or the Human Accuracy of that task<br><b>2. Data : </b> It is the main part behind the success or failure of the model. Poor Data or mis-balanced data can lead to poor performance so that should be checked. We also check if there is any noise present in the Data and if any then remove it to stop model from bad performance. Also the problem can be improper splitting of training and testing Dataset.<br><b>3. Features : </b> This is also one of the aspect for error analysis. Here we analyze the Dataset to identify the features of the Dataset that are just redundant and isn't useful to the model for final prediction. Removing such features can help improve the performance and reduce the errors. Also we can identify the features contributing the most and give more weight in model training to those features. This features relevance can be found using model interpretability.\n",
    "<br><br><b> Reference</b><br>1. <a href=\"https://neptune.ai/blog/deep-dive-into-error-analysis-and-model-debugging-in-machine-learning-and-deep-learning\"> https://neptune.ai/blog/deep-dive-into-error-analysis-and-model-debugging-in-machine-learning-and-deep-learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have finished HW3 part 2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
