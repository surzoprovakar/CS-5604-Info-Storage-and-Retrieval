{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "\n",
    "In this assignment, we will be creating ranking functions (Part 1) and investigating poitwise and pairwise learning-to-rank approaches (Part 2). \n",
    "\n",
    "The HW is due **Friday, December 3, 2021 @ 11:59 pm**. You can form teams of three students, two students, or work individually. Note that there exists no difference in terms of grading, i.e., we will grade the same for one and two person teams. **Only one of the team members needs to submit the HW.**\n",
    "\n",
    "Please submit a zip file containing the two parts of the assignment:\n",
    "\n",
    "1. **Ranking Code (47%)**: Make sure to finish the ranking.ipynb Jupyter notebook first.\n",
    "2. **LTR Code (53%)**: In the learning-to-rank.ipynb notebook, we will implement LTR models. Submit both Jupyter notebooks.\n",
    "\n",
    "\n",
    "### HW3 - Learning to Rank (53% of total HW3 grade)\n",
    "\n",
    "In the first part of this assignment, we examined various ways of ranking documents given a query; however, weights for different features were not learned automatically but set manually. As more and more ranking signals are investigated, integrating more features becomes challenging as it would be hard to come up with a single ranking function like BM25 for arbitrary features. \n",
    "\n",
    "In this assignment, you will be investigating different approaches to the learning to rank task that you have learned: (1) the pointwise approach using linear regression and (2) the pairwise approach employing gradient boosted decision trees. The goal is to let these algorithms learn weights automatically for various features. \n",
    "\n",
    "More specifically, it involves the following tasks (weights are for the programming assignment as a whole):\n",
    "* [Task 1: Pointwise Approach and Linear Regression (10%)](#Task-1:-Pointwise-Approach-and-Linear-Regression-(10%)): Implement a pointwise approach with linear regression based on basic tf-idf features\n",
    "* [Task 2: Pairwise Approach and Gradient Boosted Decision Trees (15%)](#Task-2:-Pairwise-Approach-and-Gradient-Boosted-Decision-Trees-(15%)): Implement an instance of the pairwise approach with the help of gradient boosted decision trees, using basic tf-idf features\n",
    "* [Task 3: Train Your Best Model (20%)](#Task-3:-Adding-More-Features-(20%)) Train your best model, and experiment with more features such as BM25, Smallest Window, and PageRank\n",
    "* [Task 4: Report (8%)](#Task-4:-Report-(8%)): Write up a summary report and answer some questions about the above tasks\n",
    "\n",
    "\n",
    "__Grading__\n",
    "- Part of your grade will be based on your model's performance on a hidden test set. \n",
    "- You will get full credit for solutions that receive NDCG scores within reasonable range of the NDCG scores received by the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `base_classes` folder contains useful class definitions (not to be edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('base_classes')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add additional imports below as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add additional imports here\n",
    "\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset is the same as what you used in the first part of this HW.**\n",
    "\n",
    "As in the first part of the HW, we have partitioned the data into two sets for you: \n",
    "1. Training set (hw3.(signal|rel).train)\n",
    "2. Development set (hw3.(signal|rel).dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading previous code\n",
    "\n",
    "We load the AScorer class that you completed in the first part of HW3. \n",
    "\n",
    "**Note that you may need to make updates to this class for completing the tasks in this notebook.**\n",
    "\n",
    "We also load the Idf class that you can use to get document frequency values based on the corpus. You will also need to load the Rank class for the computation of NDCG scores on the tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_classes.load_train_data import load_train_data\n",
    "from base_classes.id_map import IdMap\n",
    "from base_classes.ndcg import NDCG\n",
    "from base_classes.query import Query\n",
    "from base_classes.document import Document\n",
    "from base_classes.ascore import AScorer\n",
    "from base_classes.build_idf import Idf\n",
    "from base_classes.rank import Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Pointwise Approach and Linear Regression (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ranking, each query $q_i$ will be associated with a set of documents, and for each document $j$, we extract a query-document feature vector $x_{i,j}$. There is also a label $y_{i,j}$ associated with each query-document vector $x_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pointwise approach, such group structure in ranking is ignored, and we simply view our training data as $\\{(x_{i}, y_{i})\\}$, where each instance consists of a query-document feature vector $x_{i}$ and a label $y_{i}$ (which is a relevance score as in the first part of this programming assignment). The ranking problem amounts to learning a function $f$ such that $f(x_{i})$ closely matches $y_{i}$.\n",
    "\n",
    "In this task, we consider a very simple instance of the pointwise approach, the *linear regression* approach. That is, we will use a linear function $f$ which gives a score to each query-document feature vector $x$ as follows: $f(x) = wx+b$. Here, the weight vector ${w}$ and the bias term $b$ are parameters that we need to learn to minimize the loss function as defined below:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^m (f(x_{i})-y_{i})^2\n",
    "\\end{equation}\n",
    "This formulation is also referred to as the *ordinary least squares* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Designing Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each query-document pair as a five-dimensional vector of query vector-document vector (tf-idf) scores. Each dimension corresponds to a document field -- url, title, header, body, and anchor. Specifically, given a query vector $q$ and a document vector $d_{f}$ of a document field $f$, the tf-idf score is the dot product $q \\cdot d_{f}$. \n",
    "\n",
    "To start with, use query and document vectors with lnn.ltc weighting (as represented in SMART notation ddd.qqq). In other words, begin by using:\n",
    "\n",
    "1) For the document vectors, \"lnn\":\n",
    "    - logarithmic term frequency of query terms in documents\n",
    "    - no document frequency \n",
    "    - no normalization\n",
    "2) For the query vector, \"ltc\":\n",
    "    - logarithmic term frequency for words in query\n",
    "    - idf (inverse document frequency)\n",
    "    - cosine (i.e., L2) normalization\n",
    "    \n",
    "Then, experiment with a few weighting schemes other than lnn.ltc.  Refer to the figure below for other possible weighting schemes. You will report which weighting scheme yields the best performance in Task 4.\n",
    "\n",
    "<img src=\"fig/IIR_fig_6.15.png\">\n",
    "Figure is from Pg.128 http://nlp.stanford.edu/IR-book/pdf/06vect.pdf\n",
    "\n",
    "\n",
    "A few important notes:\n",
    "- Creating these vectors is similar to the exercise you performed in computing cosine similarity in the first part of this programming assignment\n",
    "- Make modifications to the AScorer class in order to try to implement other weighting mechanisms \n",
    "- **You will use these basic feature vectors for both Task 1 and Task 2. Do not use any other signals or features for Tasks 1 and 2; you will have the opportunity to use these features in Task 3.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features (signal_file, idf):\n",
    "    '''\n",
    "    Create a feature vector from the signal file and from the idf_dict. \n",
    "\n",
    "    Args:\n",
    "        signal_file: filepath to signal file\n",
    "        idf: object of class Idf (with idf built)\n",
    "\n",
    "    Returns:\n",
    "        feature_vec (numpy array of dimension (N, 5)): N is the number of (query, document)\n",
    "        pairs in the relevance file.\n",
    "    '''\n",
    "\n",
    "    # Experiment with different values of weighting below. Note that this uses dddqqq notation.\n",
    "    # Make sure to set weighting to the best value prior to submitting your code.\n",
    "    # You should be able to support lnn.ltc weighting, along with any other weighting that you experiment with\n",
    "\n",
    "\n",
    "    WEIGHTING = 'lnnltc' \n",
    "\n",
    "    assert len(WEIGHTING) == 6, \"Invalid weighting scheme.\"        \n",
    "\n",
    "    feature_vec = []\n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_relevance (relevance_file):\n",
    "    '''\n",
    "    Extract relevance scores from the relevance file. This should be a simple wrapper (<10 lines) over\n",
    "    the get_rel_scores() function in the NDCG class.\n",
    "\n",
    "    Args:\n",
    "        relevance_file: filepath to relevance file\n",
    "\n",
    "    Returns:\n",
    "        relevance_vec (numpy array of dimension (N,)): N is the number of (query, document)\n",
    "        pairs in the relevance file.   \n",
    "        ndcg_obj: NDCG object which contains relevance scores\n",
    "    '''  \n",
    "\n",
    "\n",
    "    relevance_vec = []\n",
    "    ndcg_obj = NDCG()\n",
    "\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    return relevance_vec, ndcg_obj   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Training a Linear Regression Model\n",
    "\n",
    "Implement the PointwiseLearner class below. You may use the LinearRegression class from the sklearn package. If you use the LinearRegression class, set fit_intercept to true and normalize to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, x, y):\n",
    "    \n",
    "        '''\n",
    "        - Train your linear regression model using the LinearRegression class \n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of query, document pairs. \n",
    "                Is the independent variable for linear regression. \n",
    "\n",
    "                y (numpy array of dimension (N,)): Relevance score for each query, document pair. \n",
    "                Is the dependent variable for linear regresion.\n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, x):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                x (numpy array of dimension (N, 5)): Feature vector for each query, document pair. \n",
    "                Dimension is N x 5, where N is the number of (query, document) pairs. \n",
    "                Predictions are made on this input feature array.\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained linear regression model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Docs is 98998\n",
      "Total Number of Terms is 347071\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0ce08fc9d276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_signal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train features are of incorrect shape. They should be 5 dimensions, but got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Train linear regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "lm = PointwiseLearner()\n",
    "\n",
    "idf = Idf()\n",
    "\n",
    "#Get train features and relevance\n",
    "\n",
    "train_signal_file = \"data/hw3.signal.train\"\n",
    "train_rel_file = \"data/hw3.rel.train\"\n",
    "train_features = get_features(train_signal_file, idf)\n",
    "train_relevance, train_ndcg = get_relevance(train_rel_file)\n",
    "assert train_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])\n",
    "\n",
    "#Train linear regression model\n",
    "\n",
    "lm.train_model(train_features, train_relevance)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "dev_signal_file = \"data/hw3.signal.dev\"\n",
    "dev_rel_file = \"data/hw3.rel.dev\"\n",
    "dev_features = get_features(dev_signal_file, idf)\n",
    "dev_relevance, dev_ndcg =  get_relevance(dev_rel_file)\n",
    "dev_predicts = lm.predict_model(dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your code passes the sanity check below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9178978bf2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdev_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train features are of incorrect shape. They should be 5 dimensions, but got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_features' is not defined"
     ]
    }
   ],
   "source": [
    "assert dev_features.shape[1] == 5, 'Train features are of incorrect shape. They should be 5 dimensions, but got {}'.format(train_predicts.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using the predictions from your trained model, compute the mean squared error and NDCG score that you receive. \n",
    "\n",
    "Include the score you received in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_calc_for_LTR (dev_ndcg, dev_predicts, out_file=\"ranked_result_default\"):\n",
    "\n",
    "    ''' We provide this function to calculate the average NDCG score given a predicted score and a ground truth score.\n",
    "        Note that the code below calls rank_with_score() in the Rank class, so the correct value for NDCG \n",
    "        depends on the correct implmementation of that function.\n",
    "         Args:\n",
    "                dev_ndcg (type NDCG): Object that contains the \"ground truth\" relevance scores in dev_ndcg.rel_scores \n",
    "                dev_predicts: numpy array of dimension (N,) which contains predicted scores for a dataset.\n",
    "                out_file: filename to which the ranked_result_file is written\n",
    "            \n",
    "        Returns: avg_ndcg_score: Scalar that averages NDCG score across all queries. \n",
    "    \n",
    "    '''\n",
    "    idx = 0\n",
    "    dev_predicts_dict = {}\n",
    "\n",
    "    #Converts the dev_predicts vector into query->url->score dict\n",
    "    for query, url_dict in dev_ndcg.rel_scores.items():\n",
    "        query_obj = Query(query) #Converts str to Query object\n",
    "        dev_curr_dict = {}\n",
    "        for url in url_dict.keys():\n",
    "            dev_curr_dict[url] = dev_predicts[idx]\n",
    "            idx+=1\n",
    "        dev_predicts_dict[query_obj] = dev_curr_dict\n",
    "\n",
    "    #Orders dev_predicts_dict. This remains a Query->url->score dict after ordering.\n",
    "    #Note that this depends on your implementation of the rank_with_score() function in the Rank class.\n",
    "    r = Rank()\n",
    "    dev_predicts_dict_ordered = r.rank_with_score(dev_predicts_dict)\n",
    "\n",
    "    #Creates a Query->Document->score dict called dev_predicts_ranks that will be written to file.\n",
    "    dev_data = load_train_data(dev_signal_file) #Query->Document dict\n",
    "\n",
    "    dev_predicts_ranked = {} #The Query->Document->Score dict that will be written to file.\n",
    "    for query in dev_predicts_dict_ordered:\n",
    "        doc_to_score = {}\n",
    "        for url in dev_predicts_dict_ordered[query]:\n",
    "            doc = dev_data[query][url]\n",
    "            doc_to_score[doc] = dev_predicts_dict_ordered[query][url]\n",
    "        dev_predicts_ranked[query] = doc_to_score\n",
    "\n",
    "    #Writes dev_predicts_ranked to file.\n",
    "    if not os.path.exists(\"output\"): os.mkdir(\"output\")\n",
    "    ranked_result_file = os.path.join(\"output\", out_file)\n",
    "    r.write_ranking_to_file(dev_predicts_ranked, ranked_result_file)\n",
    "\n",
    "    #Uses the NDCG class to get the NDCG score\n",
    "    dev_ndcg.read_ranking_calc(ranked_result_file)\n",
    "    avg_ndcg_score = dev_ndcg.get_avg_ndcg()\n",
    "    return avg_ndcg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_relevance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-aa64d7d601e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute mean squared error and NDCG Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Squared Error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_relevance' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute mean squared error and NDCG Score\n",
    "\n",
    "mse = mean_squared_error(dev_relevance, dev_predicts)\n",
    "\n",
    "print (\"Mean Squared Error:\", mse)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts, \"ranked_result_pointwise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Pairwise Approach and Gradient Boosted Decision Trees (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the LambdaMART algorithm to implement Gradient Boosted Decision Trees. \n",
    "\n",
    "LambdaMART is the boosted tree version of an earlier algorithm, LambdaRank. The full evolution of algorithms from RankNet through LambdaRANK, MART and LambdaMART is presented below (Page 16 and 17 are particularly important). \n",
    "https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf\n",
    "\n",
    "The relevant lecture notes are **CS5604LearningToRank.pdf** accessible at [Canvas Files](https://canvas.vt.edu/courses/136044/files?preview=20605783)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the XGBoost package to implement LambdaMART. You may find it helpful to read the documentation here: https://xgboost.readthedocs.io/en/latest/get_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter description (not exhaustive, see here for more details): https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "General Parameters (**make sure to use the following values**):\n",
    "- \"booster\": use \"gbtree\". Uses a tree-based model for boosting\n",
    "- \"objective\": use \"rank:pairwise\". Uses the LambdaMART algorithm to minimize pairwise loss. \n",
    "- \"eval_metric: use \"ndcg\" (while we will be evaluating your performance solely based on ndcg, feel free to test performance on other metrics)\n",
    "\n",
    "Hyperparamters to be tuned (not exhaustive):\n",
    "- \"eta\": Learning rate\n",
    "- \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "- \"max_depth\": Maximum depth of a tree\n",
    "- \"subsample\": Subsample ratio of training instances to prevent overfitting\n",
    "\n",
    "When training, you should also experiment with early stopping to prevent overfitting. Take a look at the description of early stopping here: https://xgboost.readthedocs.io/en/latest/python/python_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_signal_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-436cc8300a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Expected 700 queries, but got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdev_query_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_signal_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdev_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdev_query_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_signal_file' is not defined"
     ]
    }
   ],
   "source": [
    "train_query_dict = load_train_data(train_signal_file)\n",
    "train_groups = []\n",
    "for query, url_dict in train_query_dict.items():\n",
    "    train_groups.append(len(url_dict))\n",
    "    \n",
    "assert len(train_groups) == 700, 'Expected 700 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "dev_query_dict = load_train_data(dev_signal_file)\n",
    "dev_groups = []\n",
    "for query, url_dict in dev_query_dict.items():\n",
    "    dev_groups.append(len(url_dict))\n",
    "    \n",
    "assert len(dev_groups) == 100, 'Expected 100 queries, but got {}'.format(len(train_groups))\n",
    "\n",
    "dtrain = xgb.DMatrix(train_features, label = train_relevance)\n",
    "dtrain.set_group(train_groups)\n",
    "ddev = xgb.DMatrix(dev_features, label = dev_relevance) \n",
    "ddev.set_group(dev_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLearner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "        self.model = None\n",
    "\n",
    "    def train_model (self, dtrain, evallist):\n",
    "    \n",
    "        '''\n",
    "        - Specifies parameters for XGBoost training\n",
    "        - Trains model\n",
    "\n",
    "        Args:\n",
    "                dtrain (type DMatrix): DMatrix is a internal data structure that used by XGBoost \n",
    "                which is optimized for both memory efficiency and training speed.\n",
    "                \n",
    "                evallist (array of tuples): The datasets on which the algorithm reports performance as training takes place\n",
    "                \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        num_rounds = 10 #Experiment with different values of this parameter\n",
    "        \n",
    "        ### Begin your code\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params[\"booster\"] = \"gbtree\"\n",
    "        self.params[\"objective\"] = \"rank:pairwise\"\n",
    "        self.params[\"eval_metric\"] = \"ndcg\"\n",
    "        self.params[\"eta\"] = 1/(num_rounds*num_rounds)\n",
    "        self.params[\"gamma\"] = 1/(num_rounds*num_rounds)\n",
    "        self.params[\"max_depth\"] = num_rounds\n",
    "        self.params[\"subsample\"] = 1\n",
    "        \n",
    "        self.model = xgb.train(\n",
    "            self.params,\n",
    "            dtrain=dtrain,\n",
    "            evals=evallist,\n",
    "            early_stopping_rounds=num_rounds,\n",
    "            num_boost_round=num_rounds)\n",
    "\n",
    "        ### End your code\n",
    "    \n",
    "    def predict_model (self, dtest):\n",
    "    \n",
    "        '''\n",
    "        - Output predicted scores based on the trained model.\n",
    "\n",
    "        Args:\n",
    "                dtest (type DMatrix): DMatrix that contains the dev/test signal data\n",
    "\n",
    "        Returns:\n",
    "                y_pred (numpy array of dimension (N,)): Predicted relevance scores for each query, document pair\n",
    "                based on the trained  model.\n",
    "        '''\n",
    "        ### Begin your code\n",
    "        \n",
    "        return self.model.predict(dtest,self.model.best_ntree_limit)\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6313d9f71713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBDTLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevallist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtrain' is not defined"
     ]
    }
   ],
   "source": [
    "#Train a gradient boosted decision trees model.\n",
    "\n",
    "model = GBDTLearner()\n",
    "evallist = [(dtrain, 'train')]\n",
    "model.train_model(dtrain, evallist)\n",
    "\n",
    "# Get predictions on dev set.\n",
    "\n",
    "dev_predicts_gbdt = model.predict_model(ddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_ndcg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-201fd1fa6981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Average NDCG score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDCG_calc_for_LTR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_ndcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_predicts_gbdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ranked_result_gbdt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_ndcg' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_gbdt, \"ranked_result_gbdt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Adding More Features (20%)\n",
    "\n",
    "Putting it all together! In this part, train your best model - and feel free to use additional features! Experiment with the following to see which yields the best performance on the dev set:\n",
    "\n",
    "1. Using smallest window feature from the first part of this programming assignment\n",
    "2. Using Pagerank from the idf file\n",
    "\n",
    "In addition, you may also choose to experiment with using word vectors. We provide GLoVE embeddings for the words in our vocabulary, which you can download with the help of embedding.py in the base_classes folder. (Disclaimer: downloading might be slow)\n",
    "\n",
    "You may choose to write several helper functions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "   \n",
    "    # You may choose to write other helper functions below \n",
    "    # (such as to augment feature array with additional features)\n",
    "    \n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    \n",
    "    \n",
    "    def train_and_predict(self, train_signal_file, train_rel_file, test_signal_file, idf):\n",
    "    \n",
    "        '''\n",
    "        - Receives the training signal and relevance files as parameters\n",
    "        - Creates a feature vector associated with the signal file\n",
    "        - Trains the best possible model on the training data\n",
    "        - Using the trained model, makes a prediction on the test_signal_file\n",
    "        \n",
    "        - \n",
    "\n",
    "        Args:\n",
    "            train_signal_file: filename of training signal\n",
    "            train_rel_file: filename of training relevance file\n",
    "            test_signal_file: filename containing dev/test signal\n",
    "            idf: object of class IDF, containing a fully built idf dictionary\n",
    "            \n",
    "\n",
    "        Returns: none\n",
    "        '''\n",
    "        test_predictions = []\n",
    "    \n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestModel()\n",
    "idf = Idf()\n",
    "train_signal_file = \"data/hw3.signal.train\"\n",
    "train_rel_file = \"data/hw3.rel.train\"\n",
    "dev_signal_file = \"data/hw3.signal.dev\"\n",
    "\n",
    "dev_predicts_best = model.train_and_predict(train_signal_file, train_rel_file, dev_signal_file, idf)\n",
    "\n",
    "dev_rel_file = \"data/hw3.rel.dev\"\n",
    "dev_relevance, dev_ndcg = get_relevance(dev_rel_file)\n",
    "\n",
    "print (\"Average NDCG score:\", NDCG_calc_for_LTR(dev_ndcg, dev_predicts_best, \"ranked_result_best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Report (8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is meant to be relatively more open-ended as you describe the model choices you made in this assignment. **Please keep your report concise.** Be sure to document any design decisions you made, and provide a brief rationale for them. \n",
    "\n",
    "You may choose to insert cells below to generate tables or plots if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Design of feature vectors (Task 1 and 2) (1%)\n",
    "\n",
    "For each (query, document) pair, in designing your feature vector from query vector and document vectors, you had various possible options for (i) term frequency, (ii) document frequency and (iii) normalization. The default option we recommended you start with for the feature vector is lnn.ltc (using the SMART notation ddd.qqq).\n",
    "\n",
    "What other choices did you experiment with? How did the performance compare across these choices? What might be the rationale for this difference in performance across the various models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have used **tf-idf** as a feature vector for each query-document pair. Besides it, we experimented with raw term frequency. And from our experiment, we see that using tf-idf to design feature vector give better performance. Because while using raw term frequency, a term that has occurred 20 times doesn't mean that always the term carries twenty-time significance, there may be that less occurred term carries more significance. So just implementing a feature based on raw term frequency doesn't always guarantee good performance. But using tf-idf we have a basic metric to extract the most descriptive terms in a document and we can easily compute the similarity between two documents using it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Hyperparameter tuning  (Task 2) (1%)\n",
    "\n",
    "Briefly describe the hyperparameters you tuned for your implementation of XGBoost. \n",
    "Which hyperparameters were most consequential to the performance of the model?\n",
    "\n",
    "Provide an intuition, based on your understanding of the LambdaMART algorithm, for why the performance of the model varied as it did with the hyperparameters you tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To implement XGBoost, the hyperparameters we have used are <br>\n",
    "\"eta\" : 0.01<br> \"gamma\" : 0.01<br> \"max_depth\" : 10<br> \"subsample\" : 1<br>\n",
    "We believe the hyperparameters **eta, gamma, subsample** were most consequential to the performance of the model as eta refers learning rate of the model, gamma is the loss reduction and this value is smaller and subsample refers to ratio of the instances to prevent overfitting and the value we used didn't cause overfitting for our model.<br><br>\n",
    "From our understanding of the LambdaMart algorithm, for a given learning rate the Gradient of cross-entropy times pairwise change in target metric when the loss reduction is *1/(number of rounds)*. But we have used *1/(number of boost rounds*number of boost rounds)* and we think because of that the performance of our model varied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model Design and Ablation Analysis (Task 3) (3%)\n",
    "\n",
    "You had the option to include various additional features in your model design. Which features did you experiment with? Which features did you end up using in your final model, and why? \n",
    "\n",
    "We expect ablation analysis on which features provided useful signals and which ones did not. Please include at least 1-2 plots and/or tables for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Error Analysis (Task 3) (3%)\n",
    "\n",
    "Analyze your errors for the best performing model you trained. Please include 1-2 plots and/or tables for this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have finished HW3 part 2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
