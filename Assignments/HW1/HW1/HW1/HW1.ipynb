{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submission instructions\n",
    "\n",
    "In this programming assignment, you will create a simple search engine. More specifically, you will implement an index over a corpus of webpages, and also compress the index using variable length encoding. Both uncompressed and compressed indices will be tested on retrieval with Boolean conjunctive queries. The assignment is object-oriented and relies on python concepts such as functions, files and classes. Our TA, Makanjuola, has compiled a list of tutorials on these concepts:\n",
    "\n",
    "1. A short video: [Learn python in 1 hour](https://www.youtube.com/watch?v=kqtD5dpn9C8)\n",
    "2. [These docs](https://docs.python.org/3.7/reference/datamodel.html#special-method-names) describe special functions such as `__getitem__` , etc. \n",
    "3. This [short tutorial](https://www.omkarpathak.in/2018/04/11/python-getitem-and-setitem/) is also useful to get started with special functions.\n",
    "4. Here is a very helpful tutorial on Python [file handling](https://www.programiz.com/python-programming/file-operation).\n",
    "5. [This youtube video](https://www.youtube.com/watch?v=wfcWRAxRVBA) and [this article](https://www.w3schools.com/python/python_classes.asp) can help get you started with Classes in Python.\n",
    "6. All tutorials (and more) are categorized in this [Google doc](https://docs.google.com/document/d/1Q_aw1bW2Dx3eUMnlQLYl1p8iMyf4lXkBDjNfk688jHw/edit?usp=sharing).\n",
    "\n",
    "The HW is due **Wednesday, September 29, 2021 @ 11:59 pm**. You can form teams of two or work individually. Note that there exists no difference in terms of grading, i.e., we will grade the same for one and two person teams. Only one of the team members needs to submit the HW. \n",
    " \n",
    "\n",
    "Instructions on how to submit will be given soon. We will use an autograding system. Please do **NOT** change class and method names, otherwise autograding will fail. ONLY edit the parts with **\"### Begin your code\"** and **\"### End your code\"** comments. Note that all aspects of this assignment are covered by the [Honor Code](https://graduateschool.vt.edu/content/dam/graduateschool_vt_edu/graduate-honor-system/Constitution2021.pdf) and that the autograding system will also perform screening checks for code similarity. We will also inspect each student's code for implementation correctness. Finally, note that we will increase office hours before the HW deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare your teammate here \n",
    "Include the name and email for your team member. If you are working independenty, write \"Myself\"\n",
    "> My teammate for this HW is: `Name: Vinit Anishkumar Masrani, Email: vinitanishkumar@vt.edu `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading breakdown\n",
    "\n",
    "**Autograder (60%)**: To ensure your indices are built correctly, both the uncompressed and compressed index versions of your implementations will be tested on 20 Boolean conjunctive queries of one or multiple terms. For those queries, you will get 1.5% of the final grade for each query you answer correctly, for a total of 60% of your grade (30% for the compressed index and 30% for the uncompressed index). \n",
    "\n",
    "**Code inspection (40%)**: We will also grade each function implementation individually. The grading breakdown is as follows:\n",
    "1. `IdMap` - 4%\n",
    "2. `BSBIIndex parse_block` - 4%\n",
    "3. `InvertedIndexWriter append` - 2%\n",
    "4. `BSBIIndex invert_write` - 4%\n",
    "5. `InvertedIndexIterator` - 2%\n",
    "6. `BSBIIndex merge` - 4%\n",
    "7. `InvertedIndexMapper _get_postings_list` - 2%\n",
    "8. `sorted_intersect` - 4%\n",
    "9. `BSBIIndex retrieve` - 4%\n",
    "10. `CompressedPostings` - 4%\n",
    "11. Index size & Performance Questions (Q1-Q4) - 1.5% each question, for a total for 6% (4 x 1.5)\n",
    "\n",
    "\n",
    "Finally note that in order to get full score:\n",
    "- The `retrieve` function should NOT load the whole index into memory  but rather load the postings lists of just the query terms.\n",
    "- The `merge` function should NOT load the complete intermediate indices or keep the whole merged index in memory.\n",
    "- The `retrieve` function should order the query terms by postings list length.\n",
    "- Both the `merge` function and the `sorted_intersect` function should NOT use the built in set operations  while merging postings lists or intersecting sorted lists.\n",
    "- Timings for the `retrieve` function should be within a time range. This will be ensured if you do NOT iterate through your index instead of seeking just the query terms, as this will significantly increase the time complexity. \n",
    "- Your compression algorithm should indeed achieve a compressed index, i.e., you should see a significant index size decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a few necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus you will be working with for this assignment contains webpages from the cs.vt.edu, vt.edu, illinois.edu and cs.illinois.edu domains. The data for this assignment are included as a .zip file. There are 4 sub-directories under the data directory. We will also remove HTML tags so that each webpage is a sequence of space-delimited words. Each consecutive span of non-space characters consists of a word token in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "data_dir = 'hw1data'\n",
    "zip_ref = zipfile.ZipFile(data_dir+'.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "sorted(os.listdir('hw1data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 43.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data ...\n",
      "Going over hw1data/3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 38.27it/s]\n",
      "  4%|▍         | 4/100 [00:00<00:02, 34.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 32.16it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.25it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 38.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.05it/s]\n",
      "  2%|▏         | 2/101 [00:00<00:05, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:04<00:00, 23.74it/s]\n",
      "  4%|▍         | 4/100 [00:00<00:02, 35.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.54it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:04, 19.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going over hw1data/5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "new date().gettime(),event:'gtm.js'});var f=d.getelementsbytagname(s)[0],\n",
      "j=d.createelement(s),dl=l!='datalayer'?'&l='+l:'';j.async=true;j.src=\n",
      "'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentnode.insertbefore(j,f);\n",
      "})(window,document,'script','datalayer','gtm-3sqhdc'); (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "      new date().gettime(),event:'gtm.js'});var f=d.getelementsbytagname(s)[0],\n",
      "      j=d.createelement(s),dl=l!='datalayer'?'&l='+l:'';j.async=true;j.src=\n",
      "      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentnode.insertbefore(j,f);\n",
      "      })(window,document,'script','datalayer','gtm-p6f85q5'); apply | virginia tech skip to main content skip to search virginia tech® universal access universal access options report a barrier accessibility portal pause all background videos underline all links apply visit give shop hokie gear apparel, clothing, gear and merchandise hokie shop university bookstore, merchandise and gifts hokie license plates part of every virginia tech plate purchase funds scholarships resources for future students current students parents and families faculty and staff alumni industry and partners virginia tech menu virginia tech about facts about virginia tech campus locations university leadership arts@virginiatech history and traditions rankings academics colleges undergraduate majors undergraduate minors graduate school beyond boundaries online learning libraries continuing and professional education summer and winter sessions experiential learning living-learning programs study abroad admissions & aid cost & aid federal cares act reporting disclosure undergraduate admissions applying to virginia tech visit virginia tech transfer admissions international admissions veteran admissions graduate school admissions undergraduate admissions calendar first-generation students first-generation college institute black college institute tribal initiatives frequently asked questions online programs admissions staff site map campus life discover blacksburg housing dining career and professional development health and wellness corps of cadets clubs, organizations, and involvement cultural centers athletics inclusivevt engagement and outreach research apply visit give shop hokie gear apparel, clothing, gear and merchandise hokie shop university bookstore, merchandise and gifts hokie license plates part of every virginia tech plate purchase funds scholarships resources for future students current students parents and families faculty and staff alumni industry and partners search search submit home / apply / explore about academics admissions & aid campus life inclusivevt engagement and outreach research apply ready1 apply ready to apply? choose the experience that you're seeking. apply for undergraduate admission apply to the virginia-maryland college of veterinary medicine apply for graduate school apply to the virginia tech carilion school of medicine get directions see all locations contact virginia tech university status principles of community privacy statement acceptable use we remember university libraries accessibility consumer information stop abuse policies equal opportunity wvtf university bookstore jobs at virginia tech strategic plan © 2021 virginia polytechnic institute and state university. all rights reserved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Strip of HTML tags from all files and lowercase all words. This might take a while.\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm #progress bars\n",
    "\n",
    "new_data_dir = 'hw1dataraw'\n",
    "os.makedirs(new_data_dir, exist_ok=True)\n",
    "\n",
    "def removeTags(soup):\n",
    "    for item in soup.find_all('span', 'sr-only'):\n",
    "        item.decompose()\n",
    "    return soup\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(data_dir):\n",
    "    print(f'Going over {dirpath} ...')\n",
    "    for filename in tqdm(filenames, total=len(filenames)):\n",
    "        inF = open(os.sep.join([dirpath, filename]),'rb')\n",
    "        soup = BeautifulSoup(inF, features=\"html.parser\")\n",
    "        soup = removeTags(soup)\n",
    "        text = soup.get_text(strip=True, separator = \" \").lower()\n",
    "        newsubdir = dirpath.replace(data_dir, new_data_dir)\n",
    "        os.makedirs(newsubdir, exist_ok=True)\n",
    "        outF = open(os.sep.join([newsubdir, filename]), \"w\", encoding='utf-8')\n",
    "        outF.write(text)\n",
    "        outF.close()\n",
    "        inF.close()\n",
    "\n",
    "data_dir = new_data_dir # point data_dir to the new dir.\n",
    "filename = 'hw1dataraw/6/apply.html'\n",
    "soup = BeautifulSoup(open(filename,'r'), features=\"html.parser\")\n",
    "soup = removeTags(soup)\n",
    "print(soup.get_text(strip=True, separator = \" \").lower()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the directory for where the index will be stored (`index_dir`) and a separate folder `tmp` that will contain some temporary files. A small test corpus in subfolder `testdata` is included in the HW folder, which you can use for code testing purposes, and faster implementation cycles. The index for the `testdata` will be stored in `test_index_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('index_dir')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('test_index_dir')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an uncompressed index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment is to build an inverted index of this corpus, and implement Boolean conjunctive queries. **In particular, you need to implement the blocked sort-based indexing (BSBI) algorithm described in [Section 4.2](http://nlp.stanford.edu/IR-book/pdf/04const.pdf) of the textbook.** To fully understand the details of BSBI, we recommend that you read Section 4.2 .\n",
    "\n",
    "> Remember from lectures, to construct an index, first we assemble all term-docID pairs, where each document is represented with a unique serial numbers instead of strings. Then, we sort term-docID pairs by the term and docID. Finally, we organize docIDs for each term into a postings list and compute statistics like term and document frequency. To make the index more efficient, we can also represent terms as termIDs (unique serial numbers instead of strings). The dictionary that maps terms to termIDs can be built on the fly as we are processing the document collection. For small collections, the whole index construction can be done in memory. Here, we will implement large-scale methods that require use of secondary storage (disk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdMap\n",
    "\n",
    "Let us first build a helper class `IdMap`, which maps strings to numeric ids (and vice-versa). This will be useful for mapping terms to termIDs and docs to docIDs. Data structures used include: i) dictionary (from strings to numeric ids) and ii) lists (from numeric ids to strings).\n",
    "\n",
    "\n",
    "In the following code, fill in the functions `_get_str` and `_get_id` . Notice that accessing items can be done by `__getitem__` which gets the correct mapping depending on the type of the key. We will later on incorporate functionality to add a string if it doesn't already exist in the map.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.id_to_str:\n",
    "            self.str_to_id[s] = len(self.id_to_str)\n",
    "            self.id_to_str.append(s)\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your code passes the following test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on you can use the `testdata` to write your own test cases, if you want to make sure that your code is working as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Postings List as bytearrays\n",
    "\n",
    "In order to write and read lists of postings (docIDs) efficiently from the disk, we store them as bytearrays. The provided`UncompressedPostings` class which contains static encode and decode functions. In the next task you'll be required to implement compressed versions with the same inferface. \n",
    "\n",
    "Some links for reference:\n",
    "1. https://docs.python.org/3/library/array.html\n",
    "2. https://pymotw.com/3/array/#module-array\n",
    "3. https://realpython.com/instance-class-and-static-methods-demystified/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncompressedPostings:\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes postings_list into a stream of bytes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int] of docIDs (postings)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: bytearray representing integers in the postings_list\n",
    "        \"\"\"\n",
    "        return array.array('L', postings_list).tobytes()\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes postings_list from a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytearray representing encoded postings list as output by encode function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        decoded_postings_list: Decoded List[int] of docIDs from encoded_postings_list\n",
    "        \"\"\"\n",
    "        decoded_postings_list = array.array('L')\n",
    "        decoded_postings_list.frombytes(encoded_postings_list)\n",
    "        return decoded_postings_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how it works, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = UncompressedPostings.encode([1,2,3])\n",
    "print(x)\n",
    "print(UncompressedPostings.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index on Disk\n",
    "\n",
    "As mentioned in lectures, inverted indices utilize external sorting algorithms (that uses disk). For efficiency, any such sorting algorithm needs to minize the number of random disks seeks, i.e., sequential disk reads are much faster than random seeks.\n",
    "\n",
    "In this section we provide a base class `InvertedIndex` which would be subsequently subclassed into `InvertedIndexWriter`, `InvertedIndexIterator` and `InvertedIndexMapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"Implements efficient reads and writes of an inverted index to disk\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    postings_dict: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                number_of_postings_in_list,\n",
    "                                                length_in_bytes_of_postings_list)\n",
    "        \n",
    "        This is a dictionary that maps from termIDs to a 3-tuple of metadata that is helpful in reading \n",
    "        and writing the postings in the index file to/from disk. This mapping is supposed to be kept in memory. \n",
    "        \n",
    "        start_position_in_index_file is the position (in bytes) of the postings list in the index file\n",
    "        number_of_postings_in_list is the number of postings (docIDs) in the postings list\n",
    "        length_in_bytes_of_postings_list is the length of the byte encoding of the postings list\n",
    "    \n",
    "    terms: List[int]\n",
    "        A list of termIDs to remember the order in which terms and their postings lists were added to index. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_name, postings_encoding=None, directory=''):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_name (str): Name used to store files related to the index \n",
    "        postings_encoding: A class implementing static methods for encoding and decoding lists of integers. \n",
    "                           Default is None, which gets replaced with UncompressedPostings\n",
    "        directory (str): Directory where the index files will be stored\n",
    "        \"\"\"\n",
    "        self.index_file_path = os.path.join(directory, index_name+'.index')\n",
    "        self.metadata_file_path = os.path.join(directory, index_name+'.dict')\n",
    "        if postings_encoding is None:\n",
    "            self.postings_encoding = UncompressedPostings\n",
    "        else:\n",
    "            self.postings_encoding = postings_encoding\n",
    "        self.directory = directory\n",
    "        self.postings_dict = {}\n",
    "        self.terms = []         \n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the index_file and loads metadata upon entering the context\"\"\"\n",
    "        # Open the index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "        # Load the postings dict and terms from the metadata file\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.terms = pkl.load(f)\n",
    "            self.term_iter = self.terms.__iter__()                       \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Closes the index_file and saves metadata upon exiting the context\"\"\"\n",
    "        # Close the index file\n",
    "        self.index_file.close()\n",
    "        # Write the postings dict and terms to the metadata file\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `__enter__` and `__exit__` functions make `InvertedIndexWriter` a [context manager](https://docs.python.org/3/library/contextlib.html) that allows using the `with` statement just like native file I/O in python. \n",
    "\n",
    "```python\n",
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    # Some code here\n",
    "```\n",
    "\n",
    "Additional references on context managers in python (although thorough understanding is not neeed for this HW):\n",
    "1. https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/\n",
    "2. http://arnavk.com/posts/python-context-managers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "BSBI segments the collection into blocks of equal size, sorts the termID-docID pairs of each part in memory, and stores intermediate sorted results on disk. Finally, all intermediate results are merged as the final index. \n",
    "\n",
    "Here, we treat each sub-directory as a block, i.e.,  small enough to be stored in memory, and only load one block in memory at a time when we build the index. We will build upon the `BSBIIndex` class. The `index` function provides the skeleton for BSBI. \n",
    "\n",
    "The next HW task is to implement `parse_block`, `invert_write` and `merge` functions in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex:\n",
    "    \"\"\" \n",
    "    Attributes\n",
    "    ----------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    output_dir(str): Path to output index files\n",
    "    index_name(str): Name assigned to index\n",
    "    postings_encoding: Encoding used for storing the postings. The default (None) implies UncompressedPostings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir, index_name = \"BSBI\", postings_encoding = None):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding\n",
    "        # Stores names of intermediate indices\n",
    "        self.intermediate_indices = []\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads doc_id_map and term_id_map from output directory\"\"\"\n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "            \n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        This function loops through the data directories, calls parse_block to parse the documents, \n",
    "        calls invert_write, which inverts each block and writes to a new index and then saves the \n",
    "        id maps and calls merge on the intermediate indices.\n",
    "        \"\"\"\n",
    "        for block_dir_relative in sorted(next(os.walk(self.data_dir))[1]):\n",
    "            td_pairs = self.parse_block(block_dir_relative)\n",
    "            index_id = 'index_'+block_dir_relative\n",
    "            self.intermediate_indices.append(index_id)\n",
    "            with InvertedIndexWriter(index_id, directory=self.output_dir, \n",
    "                                     postings_encoding=self.postings_encoding) as index:\n",
    "                self.invert_write(td_pairs, index)\n",
    "                td_pairs = None\n",
    "        self.save()\n",
    "        with InvertedIndexWriter(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=self.postings_encoding) as merged_index:\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=self.output_dir, \n",
    "                                          postings_encoding=self.postings_encoding)) \n",
    "                 for index_id in self.intermediate_indices]\n",
    "                self.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "The function `parse_block` parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full. Here, we treat each sub-directory as a block (`parse_block` expects the path to the sub-directory). You can assume that the full path of the files are unique. That means that while the individual file names are unique within each sub-directory, this may not necessarily be the case across sub-directories. \n",
    "\n",
    "*Disclaimer: it may seem that `BSBIIndex` inherits `BSBIIndex` (same class), but this is just a convenient way to split class definitions into two parts, such that we can add a new method to an existing class. This note is to avoid confusion on how we used the same name for two classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):            \n",
    "    def parse_block(self, block_dir_relative):\n",
    "        \"\"\"Parses a tokenized text file into termID-docID pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        block_dir_relative : str Relative Path to the directory that contains the files for the block\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        parseLst: List[Tuple[Int, Int]] \n",
    "        Returns all the termID-docID pairs extracted from the block\n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        These persist across calls to parse_block\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        parseList = []\n",
    "        \n",
    "        \n",
    "        filepath = self.data_dir + '/' + block_dir_relative\n",
    "        \n",
    "        myfilepathlists = os.listdir(filepath)\n",
    "        \n",
    "        for myfilepathlist in myfilepathlists:\n",
    "            fullfilepath = filepath + '/' + myfilepathlist\n",
    "            document = block_dir_relative + '/' + myfilepathlist\n",
    "            document_id = self.doc_id_map[document]\n",
    "            with open(fullfilepath, 'r', encoding=\"utf8\") as file:\n",
    "                for lines in file:\n",
    "                    for words in lines.split():\n",
    "                        termID = self.term_id_map[words]\n",
    "                        parseList.append((termID, document_id))    \n",
    "#         print(parseList)\n",
    "        return parseList\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the function works as expected on the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm fine , thank you\n",
      "\n",
      "hi hi\n",
      "how are you ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('testdata/0/fine.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "with open('testdata/0/hello.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='testdata', output_dir = 'tmp/', index_name = 'test')\n",
    "aa = BSBI_instance.parse_block('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does `parse_block` work as expected on a block of the `testdata`? \n",
    "Write a test to make sure that a given word gets the same id each time it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "BSBI_instance = BSBIIndex(data_dir='testdata', output_dir = 'tmp/', index_name = 'test')\n",
    "aa = BSBI_instance.parse_block('1')\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion\n",
    "\n",
    "However, we first implement the class `InvertedIndexWriter`, which provides an append function, just like appending to a list, with the difference that the postings_list isn't stored in memory but is directly written to disk. Next, we will add a function `invert_write` that inverts a block and writes it to disk. Inversion involves first sorting the termID-docID pairs and then collect all of pairs with the same termID into a postings list, where a posting is simply a docID. The result is an inverted index for the block, which is finally written to disk. You might find it helpful to read the [Python I/O docs](https://docs.python.org/3/tutorial/inputoutput.html) for information about appending to the end of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')\n",
    "        return self\n",
    "\n",
    "    def append(self, term, postings_list):\n",
    "        \"\"\"Appends the term and postings_list to end of the index file.\n",
    "        \n",
    "        This function should:\n",
    "        1. Encode the postings_list using self.postings_encoding\n",
    "        2. Store metadata in the form of self.terms and self.postings_dict\n",
    "           Note that self.postings_dict maps termID to a tuple of \n",
    "           (start_position_in_index_file, number_of_postings_in_list, length_in_bytes_of_postings_list)\n",
    "        3. Appends the bytestream to the index file on disk\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term: term or termID is the unique identifier for the term\n",
    "        postings_list: List[Int] of docIDs where the term appears\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        encoded_postings_list = self.postings_encoding.encode(postings_list)\n",
    "        self.terms.append(term)\n",
    "        self.postings_dict[term] = (self.index_file.tell(), len(postings_list), len(encoded_postings_list))\n",
    "        self.index_file.write(encoded_postings_list)\n",
    "#         print(encoded_postings_list)\n",
    "#         print(self.postings_dict)\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you pass the following asserts before moving forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement `invert_write`, which takes as input the termID-docID pairs created with parse_block and writes them to the given index directory, by using `InvertedIndexWriter` to write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):\n",
    "    def invert_write(self, td_pairs, index):\n",
    "        \"\"\"Inverts termID-docID pairs into postings_lists and writes them to the given index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        td_pairs: List[Tuple[Int, Int]]  (List of termID-docID pairs)\n",
    "        index: InvertedIndexWriter  (Inverted index on disk corresponding to the block)   \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        td_pairs.sort()\n",
    "        inverted_td_pairs = {}\n",
    "        for td_pair in td_pairs:\n",
    "#             if td_pair[0] in inverted_td_pairs:\n",
    "#                 if td_pair[1] not in inverted_td_pairs[td_pair[0]]:\n",
    "#                     inverted_td_pairs[td_pair[0]].append(td_pair[1])\n",
    "#             else:\n",
    "#                 inverted_td_pairs[td_pair[0]] = td_pair[1]\n",
    "            if td_pair[0] not in inverted_td_pairs:\n",
    "                inverted_td_pairs[td_pair[0]] = []\n",
    "            if td_pair[1] in inverted_td_pairs[td_pair[0]]:\n",
    "                continue\n",
    "            inverted_td_pairs[td_pair[0]].append(td_pair[1])\n",
    "#             index.append(term = td_pair[0], postings_list = inverted_td_pairs[td_pair[0]])\n",
    "        for inverted_td_pair in inverted_td_pairs:\n",
    "            index.append(term = inverted_td_pair, postings_list = inverted_td_pairs[inverted_td_pair])\n",
    "#             print(\"term\", inverted_td_pair, \"dictionary\", inverted_td_pairs[inverted_td_pair])\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some tests as you did for the `InvertedIndexWriter`, to test this on a block of the `testdata` (using the `tmp` directory) and see what the inverted index contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 1, 8),\n",
       " 1: (8, 1, 8),\n",
       " 2: (16, 1, 8),\n",
       " 3: (24, 1, 8),\n",
       " 4: (32, 2, 16),\n",
       " 5: (48, 1, 8),\n",
       " 6: (56, 1, 8),\n",
       " 7: (64, 1, 8),\n",
       " 8: (72, 1, 8)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    BSBI_instance = BSBIIndex(data_dir='testdata', output_dir = 'tmp/', index_name = 'test')\n",
    "    BSBI_instance.invert_write(BSBI_instance.parse_block('0'), index)\n",
    "index.postings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "Now we need to merge the ten blocks into one large merged index. We open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. We can iterate through the file while reading just one postings list at a time from the disk. We subclass `InvertedIndex` into `InvertedIndexIterator` to construct this iterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexIterator(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        \"\"\"Adds an initialization_hook to the __enter__ function of super class\n",
    "        \"\"\"\n",
    "        super().__enter__()\n",
    "        self._initialization_hook()\n",
    "        return self\n",
    "\n",
    "    def _initialization_hook(self):\n",
    "        \"\"\"Use this function to initialize the iterator\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        self.index_file.seek(0)\n",
    "        self.termiteration=0\n",
    "        ### End your code\n",
    "\n",
    "    def __iter__(self): \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Returns the next (term, postings_list) pair in the index.\n",
    "        Note: This function should only read a small amount of data from the index file. \n",
    "        In particular, you should not try to maintain the full index file in memory.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if self.termiteration < len(self.terms):\n",
    "            self.termvalue = self.terms[self.termiteration]\n",
    "            [self.index_file_start_index, self.len_postings_list, self.len_bytes] = self.postings_dict[self.termvalue]\n",
    "            self.index_file.seek(self.index_file_start_index)\n",
    "            self.postings_list_read_value = self.index_file.read(self.len_bytes)\n",
    "            self.postings_list_decoded_value = self.postings_encoding.decode(self.postings_list_read_value)\n",
    "            term_post_list = [self.termvalue, self.postings_list_decoded_value]\n",
    "            self.termiteration = self.termiteration + 1\n",
    "            return term_post_list\n",
    "        else:\n",
    "            return []\n",
    "        ## End your code\n",
    "\n",
    "    def delete_from_disk(self):\n",
    "        \"\"\"Marks the index for deletion upon exit. Useful for temporary indices\n",
    "        \"\"\"\n",
    "        self.delete_upon_exit = True\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Delete the index file upon exiting the context along with the \n",
    "        functions of the super class __exit__ function\"\"\"\n",
    "        self.index_file.close()\n",
    "        if hasattr(self, 'delete_upon_exit') and self.delete_upon_exit:\n",
    "            os.remove(self.index_file_path)\n",
    "            os.remove(self.metadata_file_path)\n",
    "        else:\n",
    "            with open(self.metadata_file_path, 'wb') as f:\n",
    "                pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this. Write a test that constructs a small inverted index, writes them to disk (in `tmp` folder) with an `InvertedIndexWriter`, then uses an `InvertedIndexIterator` to iterate over the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [2, 3, 4]]\n",
      "[2, [3, 4, 5]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with InvertedIndexWriter('test1', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "with InvertedIndexIterator('test1', directory = 'tmp/') as index:\n",
    "    myiter = iter(index)\n",
    "    print(next(myiter))\n",
    "    print(next(myiter))\n",
    "    print(next(myiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, [0]]\n",
      "[1, [0]]\n",
      "[2, [0]]\n",
      "[3, [0]]\n",
      "[4, [0, 1]]\n",
      "[5, [1]]\n",
      "[6, [1]]\n",
      "[7, [1]]\n",
      "[8, [1]]\n"
     ]
    }
   ],
   "source": [
    "with InvertedIndexIterator('test', directory='tmp/') as ite:\n",
    "    for _ in range(len(ite.terms)):\n",
    "        print(ite.__next__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During merging, in each iteration, we select the lowest termID that has not been processed yet (using a priority queue or a similar data structure). All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary. We'll use the `InvertedIndexIterator` to do the reading part and `InvertedIndexWriter` to write the merged postings list. \n",
    "\n",
    "Your task is to write the logic for merging *opened* `InvertedIndexIterator` objects and writing one postings list at a time into into a single `InvertedIndexWriter` object. \n",
    "\n",
    "Since we know that the postings are sorted, we can appropriately merge them in sorted order in linear time. In fact `heapq` ([documentation](https://docs.python.org/3.0/library/heapq.html)) is a standard python module that provides an implementation of the heap queue algorithm. In particular it contains a `heapq.merge` utility function which merges multiple sorted inputs into a single sorted output and returns an iterator over the sorted values. Not only can this be handy with merging postings lists, but also with merging the inverted indices. To get you started on using the `heapq.merge` function, we've provided a sample usage of the function. The two lists contain animals/birds sorted by their average life span. We want to merge the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Giraffe', 28), ('Rhinoceros', 40), ('Indian Elephant', 44), ('Indian Elephant', 70), ('Golden Eagle', 80)], [('Gray Birch', 50), ('Black Willow', 70), ('Basswood', 100), ('Bald Cypress', 600)]]\n",
      "('Giraffe', 28)\n",
      "('Rhinoceros', 40)\n",
      "('Indian Elephant', 44)\n",
      "('Gray Birch', 50)\n",
      "('Indian Elephant', 70)\n",
      "('Black Willow', 70)\n",
      "('Golden Eagle', 80)\n",
      "('Basswood', 100)\n",
      "('Bald Cypress', 600)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "animal_lifespans = [('Giraffe', 28), ('Rhinoceros', 40), ('Indian Elephant', 44), ('Indian Elephant', 70), ('Golden Eagle', 80)]\n",
    "tree_lifespans = [('Gray Birch', 50), ('Black Willow', 70), ('Basswood', 100), ('Bald Cypress', 600)]\n",
    "lifespan_lists = [animal_lifespans, tree_lifespans]\n",
    "print(lifespan_lists)\n",
    "for merged_item in heapq.merge(*lifespan_lists, key=lambda x: x[1]):\n",
    "    print(merged_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `*` to unpack `lifespan_lists` as arguments and the `lambda` function to represent the key based on which sorting is performed. \n",
    "\n",
    "We list here some references and tutorials:\n",
    "1. Unpacking lists [python documentation](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) and a [tutorial](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/)\n",
    "2. Lambda expressions [python documentation](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) and a [tutorial](https://www.afternerd.com/blog/python-lambdas/)\n",
    "\n",
    "Now complete the `merge` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import heappush, heappop, merge\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def merge(self, indices, merged_index):\n",
    "        \"\"\"Merges multiple inverted indices into a single index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: List[InvertedIndexIterator]\n",
    "            A list of InvertedIndexIterator objects, each representing an iterable inverted index for a block\n",
    "        merged_index: InvertedIndexWriter\n",
    "            An instance of InvertedIndexWriter into which each merged postings list is written out one at a time\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        indicesheapqueue = []\n",
    "        for index in indices:\n",
    "            myiter = iter(index)\n",
    "            value = next(myiter)\n",
    "            if(len(value) != 0):\n",
    "                value.append(myiter)\n",
    "                vallst = list(value)\n",
    "                heappush(indicesheapqueue, vallst)\n",
    "#         print(indicesheapqueue)\n",
    "                \n",
    "        first_inverted_index = heappop(indicesheapqueue)\n",
    "        current_index_term_id = first_inverted_index[0]\n",
    "        merged_index_postings_list = [first_inverted_index[1]]\n",
    "        myiter = iter(first_inverted_index[2])\n",
    "        value = next(myiter)\n",
    "        if len(value) != 0:\n",
    "            value.append(myiter)\n",
    "            vallst = list(value)\n",
    "            heappush(indicesheapqueue, vallst)\n",
    "#         print(indicesheapqueue)\n",
    "            \n",
    "        while len(indicesheapqueue) != 0:\n",
    "#             print(indicesheapqueue)\n",
    "            inverted_index_item = heappop(indicesheapqueue)\n",
    "            if(current_index_term_id == inverted_index_item[0]):\n",
    "                merged_index_postings_list.append(inverted_index_item[1])\n",
    "                myiter = iter(inverted_index_item[2])\n",
    "                value = next(myiter)\n",
    "                if len(value) != 0:\n",
    "                    value.append(myiter)\n",
    "                    vallst = list(value)\n",
    "                    heappush(indicesheapqueue, vallst)\n",
    "                if len(indicesheapqueue) == 0:\n",
    "                    sorted_merged_posting_list = list(heapq.merge(*merged_index_postings_list))\n",
    "                    merged_index.append(inverted_index_item[0], sorted_merged_posting_list)\n",
    "            else:\n",
    "                previous_index_term_id = current_index_term_id\n",
    "                current_index_term_id = inverted_index_item[0]\n",
    "                heappush(indicesheapqueue, inverted_index_item)\n",
    "                sorted_merged_posting_list = list(heapq.merge(*merged_index_postings_list))\n",
    "#                 print(previous_index_term_id,sorted_merged_posting_list)\n",
    "                merged_index.append(previous_index_term_id, sorted_merged_posting_list)\n",
    "                merged_index_postings_list=[]\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it works without errors on `testdata` (using `test_index_dir` as output directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='testdata', output_dir = 'test_index_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets index the full corpus (using `index_dir` as output directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=data_dir, output_dir = 'index_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you face any issues or errors with the merging part, use the following code to debug it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=data_dir, output_dir = 'index_dir', )\n",
    "BSBI_instance.intermediate_indices = [f'index_{i}' for i in os.listdir(data_dir)]\n",
    "with InvertedIndexWriter(BSBI_instance.index_name, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding) as merged_index:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding)) for index_id in BSBI_instance.intermediate_indices]\n",
    "        BSBI_instance.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean conjunctive retrieval\n",
    "\n",
    "Your task here is to implement a function `retrieve` to BSBIIndex, which given a query string consisting of space-delimited tokens, returns a list of documents that contain each of the tokens in the query. However, note that we do not want to be iterating through the index, nor loading the entire index to find the relevant terms. First we'll implement `InvertedIndexMapper` which subclasses `InvertedIndex` to add functionality for retrieving postings corresponding to a particular term by seeking to that location in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexMapper(InvertedIndex):\n",
    "    def __getitem__(self, key):\n",
    "        return self._get_postings_list(key)\n",
    "    \n",
    "    def _get_postings_list(self, term):\n",
    "        \"\"\"Gets a postings list (of docIds) for `term`. This function SHOULD NOT iterate through the index file, \n",
    "        but instead read only the bytes from the index file corresponding to the postings list for the requested term.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        [self.index_file_start_index, self.len_postings_list, self.len_bytes] = self.postings_dict[term]\n",
    "        self.index_file.seek(self.index_file_start_index)\n",
    "        self.postings_list_read_value = self.index_file.read(self.len_bytes)\n",
    "        self.postings_list_decoded_value = self.postings_encoding.decode(self.postings_list_read_value)\n",
    "        return self.postings_list_decoded_value\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few tests to check your `_get_postings_list` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 210, 211, 212, 213, 214, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 240, 244, 246, 247, 248, 253, 255, 257, 258, 259, 261, 262, 263, 266, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 293, 294, 295, 297, 299]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "with InvertedIndexMapper('BSBI', directory='index_dir/') as index:\n",
    "    x = index.__getitem__(2)\n",
    "    print(x)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain postings lists corresponding to the query terms, but to create a boolean query, we need to intersect them. We can use the fact that these lists are pre-sorted to intersect them efficiently. \n",
    "\n",
    "Your next task is to implement the `sorted_intersect` function that takes two sorted lists and returns a sorted intersection of the elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_intersect(list1, list2):\n",
    "    \"\"\"Intersects two (ascending) sorted lists and returns the sorted result\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: List[Comparable]\n",
    "    list2: List[Comparable]\n",
    "    Sorted lists to be intersected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Comparable] Sorted intersection        \n",
    "    \"\"\"\n",
    "    ### Begin your code\n",
    "    i,j = 0,0\n",
    "    list3 = []\n",
    "    while (i < len(list1)) and (j < len(list2)):\n",
    "        if(list1[i] < list2[j]):\n",
    "            i = i + 1\n",
    "        elif(list2[j] < list1[i]):\n",
    "            j = j + 1\n",
    "        else:\n",
    "            list3.append(list2[j])\n",
    "            i = i + 1\n",
    "            j = j + 1\n",
    "    return list3\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write a few test cases to check that it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 9, 30]\n"
     ]
    }
   ],
   "source": [
    "### Begin your code\n",
    "l1 = [1,3,9,20,30,50]\n",
    "l2 = [2,3,7,8,9,15,30]\n",
    "\n",
    "l3 = sorted_intersect(l1, l2)\n",
    "print(l3)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the task is to write the `retrieve` function using `InvertedIndexMapper` and `sorted_intersect`.\n",
    "Note that `retrieve` should NOT throw errors for terms not in corpus. You can take a look at exception handling in python [here](https://docs.python.org/3/tutorial/errors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):\n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Retrieves the documents corresponding to the conjunctive query\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str  Space separated list of query tokens\n",
    "            \n",
    "        Result\n",
    "        ------\n",
    "        List[str] Sorted list of documents which contains each of the query tokens. \n",
    "        Should be empty if no documents are found. Should NOT throw errors for terms not in corpus\n",
    "        \"\"\"\n",
    "        if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "            self.load()\n",
    "\n",
    "        ### Begin your code\n",
    "        query_terms = query.split()\n",
    "        results_postings_list = []\n",
    "        queried_doc_name_list = []\n",
    "        for query_term in query_terms:\n",
    "            print(self.term_id_map[query_term])\n",
    "            if query_term in self.term_id_map.id_to_str:\n",
    "                with InvertedIndexMapper(index_name = self.index_name, directory = self.output_dir, postings_encoding = self.postings_encoding) as index:\n",
    "#                     print(index.__getitem__(self.term_id_map[query_term]))\n",
    "                    results_postings_list.append(index.__getitem__(self.term_id_map[query_term]))\n",
    "            else:\n",
    "                print(\"Term not found in the Corpus . \")\n",
    "                \n",
    "        if(len(results_postings_list) != 0):\n",
    "#             print(results_postings_list)\n",
    "            while(len(results_postings_list) != 1):\n",
    "                l1 = results_postings_list.pop()\n",
    "                l2 = results_postings_list.pop()\n",
    "                l3 = sorted_intersect(l1, l2)\n",
    "                results_postings_list.append(l3)\n",
    "#                 print(results_postings_list)\n",
    "            results_postings_list = results_postings_list[0]\n",
    "            for docid_results_postings_list in results_postings_list:\n",
    "#                 print(self.doc_id_map[docid_results_postings_list])\n",
    "                queried_doc_name_list.append(self.doc_id_map[docid_results_postings_list])\n",
    "            return queried_doc_name_list\n",
    "                \n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test if our index works on the real corpus with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "2292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0/dskatz',\n",
       " '0/celebration',\n",
       " '0/featured-lectures',\n",
       " '0/department-college-and-campus-events',\n",
       " '1/values',\n",
       " '1/nskim',\n",
       " '1/rutameht',\n",
       " '2/theory-and-algorithms',\n",
       " '2/honor-code',\n",
       " '2/jugal',\n",
       " '3/fall-career-fair.html',\n",
       " '3/cs-source-events.html',\n",
       " '3/featured-spotlight.html',\n",
       " '3/index.html',\n",
       " '4/site-map.html',\n",
       " '4/property-management.html',\n",
       " '5/graduate.html',\n",
       " '6/construction-engineering-and-management.html']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=data_dir, output_dir = 'index_dir', )\n",
    "BSBI_instance.retrieve('career fair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if indeed one of the retrieved pages contains the query terms, by reading a file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "      new date().gettime(),event:'gtm.js'});var f=d.getelementsbytagname(s)[0],\n",
      "      j=d.createelement(s),dl=l!='datalayer'?'&l='+l:'';j.async=true;j.src=\n",
      "      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentnode.insertbefore(j,f);\n",
      "      })(window,document,'script','datalayer','gtm-p6f85q5'); cs|source offers in-person and virtual connections with companies   | computer science | virginia tech skip to main content skip to search virginia tech® universal access universal access options report a barrier accessibility portal pause all background videos underline all links apply visit give shop hokie gear apparel, clothing, gear and merchandise hokie shop university bookstore, merchandise and gifts hokie license plates part of every virginia tech plate purchase funds scholarships resources for future students current students parents and families faculty and staff alumni industry and partners computer science menu college of engineering  / department of computer science home about giving to cs contact us press releases undergraduate programs majors and minors courses undergraduate advising force add procedures undergraduate handbook prospective students transfer students computer science ambassadors graduate programs contacts and roles degrees and certificates administrivia graduate courses cs graduate advising compact cs grad council for prospective students funding opportunities faqs past phd recipients northern virginia programs forms new student faq covid-19 faq graduate fellowships spring 22 csa graduate force add page northern virginia center people department leadership research centers and institutes blockchain blockchain challenge 2020 blockchain boot camp blockchain challenge winners 2020 blockchain boot camp 2020 agenda blocksburg summit 2019 contact us blockchain resource portal blockchain adjunct blocksburg 2021 blockchain webinar series blockchain certificate program news and events news from computer science department spotlights the network newsletter seminars and distinguished lectures prospective student information sessions commencement spring 2020 2020 virtual awards banquet recognitions virtual town hall commencement spring 2021 2021 virtual awards banquet recognitions community diversity in computer science student groups tributes to beloved academic advisor terry arthur cs|source membership events portal student success alumni engagement alumni events alumni advisory board alumni in the news alumni awards 50 years of computer science network newsletter students in distress apply visit give shop hokie gear apparel, clothing, gear and merchandise hokie shop university bookstore, merchandise and gifts hokie license plates part of every virginia tech plate purchase funds scholarships resources for future students current students parents and families faculty and staff alumni industry and partners search search submit college of engineering / department of computer science / news and events / department spotlights / cs|source offers in-person and virtual connections with companies / explore news from computer science department spotlights chapman pendery embraces questions to create community vt international collegiate programming contest team advances to national championship cs|source stakeholder spotlight: reinventing geospatial, inc. (rgi) faculty answer the covid-19 research call a convergence of computer science talent leads to development of coronavirus tracking website living his best grad life: kobla setor zilevu ut prosim is the foundation of  emma meno’s passion for cybersecurity a successful mission for senior sam imamov cs|source stakeholder spotlight: peraton department of computer science hosts virtual town hall congratulations to 2020 dean's award recipients computer science welcomes eight new faculty members advisory board of national leaders will guide virginia tech innovation campus virtual showcase of creative computing studio 2020 fall meet fall 2020 hokie grad: samie amriui meet fall 2020 hokie grad: allison desantis stephen edwards recognized for outstanding contribution to computer science education cs|source stakeholder spotlight: caci student creativity shines in computing studio virtual showcase works of wonder student virtual showcase grady booch meets with software engineering capstone students cs|source stakeholder spotlight: mitre hoda eldardiry receives early career science award from purdue university kirk cameron named 2021 ieee computer society fellow jared cooper has a knack for making lasting impressions mostafa mohammed says the joy he receives from teaching is beyond any description chapman pendery named outstanding senior in computer science 2021 virtual awards banquet recognitions tanvi haldankar's receives undergraduate research excellence awards bethlehem teshome shines as an undergraduate teaching assistant sai gurrapu's curiosity continues to fuel his entrepreneurial drive daphne yao recognized for pioneering contributions to the cybersecurity industry alumna jamika burge receives richard a. tapia award roxanne paul honored with virginia louise \"jenny\" frank award for outstanding contributions a summer internship to remember department welcomes 11 new faculty members for academic year student apps that make the grade cs|source stakeholder spotlight: costar group cs|source offers in-person and virtual connections with companies the network newsletter seminars and distinguished lectures prospective student information sessions commencement spring 2020 2020 virtual awards banquet recognitions virtual town hall commencement spring 2021 2021 virtual awards banquet recognitions cs|source offers in-person and virtual connections with companies august 9, 2021 the department of computer science at virginia tech, through its tech-talent community, cs|source , provides a pathway for department-industry partnerships, including its career fairs held during the fall and spring semesters. the department benefits through investments of funding toward student scholarships, groups, and activities; development of students leading to internships and careers; and research opportunities to address and advance technological solutions. this year, 48 students received scholarships through the cs|source totaling $100k, all made possible through this partnership. a total of 42 companies are participating in the cs|source fall career fair, with 12 companies in person and 27 virtual, which kicked off its activities this week with a virtual career fair prep networking event on september 8. the in-person and virtual career fair will be held on september 13. the cs|source also welcomes five new companies this year, including cgi, drt strategies, kentik, mosaic learning, inc., and west creek financial. tags blacksburg main campus 1160 torgersen hall 620 drillfield drive blacksburg, va 24061 united states (540) 231-6931 blacksburg corporate research center 2202 kraft drive blacksburg, va 24060 united states (540) 231-9195 national capital region 7054 haycock road falls church, va 22043 united states (703) 538-8370 csundergrad@cs.vt.edu (undergraduate program) gradinfo@cs.vt.edu (graduate program) webmaster@cs.vt.edu (webmaster) cs intranet $(\"a\").each(function() {\n",
      "    $(this).attr(\"href\", $(this).attr(\"href\").replace(\"website.cs.vt.edu\", \"cs.vt.edu\")); }); var _gaq = _gaq || [];\n",
      "  _gaq.push(['_setaccount', 'ua-47559051-3']);\n",
      "  _gaq.push(['_trackpageview']);\n",
      "  (function() {\n",
      "    var ga = document.createelement('script'); ga.type = 'text/javascript'; ga.async = true;\n",
      "    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n",
      "    var s = document.getelementsbytagname('script')[0]; s.parentnode.insertbefore(ga, s);\n",
      "  })(); get directions see all locations contact virginia tech university status principles of community privacy statement acceptable use we remember university libraries accessibility consumer information stop abuse policies equal opportunity wvtf university bookstore jobs at virginia tech strategic plan © 2021 virginia polytechnic institute and state university. all rights reserved.\n"
     ]
    }
   ],
   "source": [
    "with open(\"hw1dataraw/3/fall-career-fair.html\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a compressed index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a compressed index using gap encoding with variable byte encoding for each gap.\n",
    "\n",
    "First, let's take a look at some useful python math operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % 2 =  0\n",
      "10 % 3 =  1\n",
      "10 / 3 =  3.3333333333333335\n",
      "10 // 3 =  3\n"
     ]
    }
   ],
   "source": [
    "# Remainder (modulo) operator %\n",
    "print(\"10 % 2 = \", 10 % 2)\n",
    "print(\"10 % 3 = \", 10 % 3)\n",
    "\n",
    "# Integer division in Python 3 is done by using two slash signs\n",
    "print(\"10 / 3 = \", 10 / 3)\n",
    "print(\"10 // 3 = \", 10 // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your taks is to fill in the following `CompressedPostings` class which we'll use as a drop-in replacement for `UncompressedPostings`. To understand in detail gap encoding (with variable byte encoding for each gap), we suggest that you revisit the corresponding lecture video and [Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf) of the textbook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitstring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-84545f7cb444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCompressedPostings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#If you need any extra helper methods you can add them here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m### Begin your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mbitstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBitArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstring_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspostinglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-84545f7cb444>\u001b[0m in \u001b[0;36mCompressedPostings\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#If you need any extra helper methods you can add them here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m### Begin your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mbitstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBitArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstring_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspostinglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspostinglist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspostinglist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyteorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'big'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitstring'"
     ]
    }
   ],
   "source": [
    "class CompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    from bitstring import BitArray\n",
    "    def string_to_bytes(spostinglist):\n",
    "        return int(spostinglist,2).to_bytes(len(spostinglist) //8, byteorder = 'big')\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` using gap encoding with variable byte encoding for each gap\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]  The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: Bytes reprsentation of the compressed postings list (as produced by `array.tobytes` function)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        ###Find Gaps\n",
    "        initialDocId = 0\n",
    "        gaps_postings_list = []\n",
    "        for docIds in postings_list:\n",
    "#             print(\"docIds\",docIds)\n",
    "            docIdsGap = docIds - initialDocId\n",
    "            gaps_postings_list.append(docIdsGap)\n",
    "#             print(\"Gap\",docIdsGap)\n",
    "            initialDocId = docIds\n",
    "#             print(\"New Initial\", initialDocId)\n",
    "#         print(gaps_postings_list)\n",
    "#         print(len(gaps_postings_list))\n",
    "        ###Now Encode Gaps by Variable Byte Encoding Method\n",
    "        string_compressed_postings_list = ''\n",
    "        for gapDocsId in gaps_postings_list:\n",
    "            binaryGapDocsId = format(gapDocsId, \"b\")\n",
    "            string_compressed_doc_id = ''\n",
    "            while len(binaryGapDocsId) > 7:\n",
    "                string_compressed_doc_id = string_compressed_doc_id + '0' + binaryGapDocsId[len(binaryGapDocsId)-7:]\n",
    "                binaryGapDocsId = binaryGapDocsId[:len(binaryGapDocsId)-7]\n",
    "            lengthZeroRequired = 7 - len(binaryGapDocsId)\n",
    "            string_compressed_doc_id = string_compressed_doc_id + '1'\n",
    "            while lengthZeroRequired > 0:\n",
    "                string_compressed_doc_id = string_compressed_doc_id + '0'\n",
    "                lengthZeroRequired = lengthZeroRequired - 1\n",
    "            string_compressed_doc_id = string_compressed_doc_id + binaryGapDocsId\n",
    "            string_compressed_postings_list = string_compressed_postings_list + string_compressed_doc_id\n",
    "#         print(string_compressed_postings_list)\n",
    "        bytes_compressed_postings_list = CompressedPostings.string_to_bytes(string_compressed_postings_list)\n",
    "        return bytes_compressed_postings_list\n",
    "        ### End your code\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int] Decoded postings list (each posting is a docIds)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        gaps_postings_list = []\n",
    "        first = \"\"\n",
    "        binaryGapDocsId = \"\"\n",
    "        string_compressed_postings_list = CompressedPostings.BitArray(bytes = encoded_postings_list)\n",
    "        string_compressed_postings_list = string_compressed_postings_list.bin\n",
    "#         print(string_compressed_postings_list)\n",
    "        while(len(string_compressed_postings_list) > 0):\n",
    "            first = string_compressed_postings_list[0]\n",
    "            binaryGapDocsId = string_compressed_postings_list[1:8]\n",
    "            string_compressed_postings_list = string_compressed_postings_list[8:]\n",
    "            while first == '0':\n",
    "                first = string_compressed_postings_list[0]\n",
    "                binaryGapDocsId = string_compressed_postings_list[1:8] + binaryGapDocsId\n",
    "                string_compressed_postings_list = string_compressed_postings_list[8:]\n",
    "            gapDocsId = int(binaryGapDocsId, 2)\n",
    "            gaps_postings_list.append(gapDocsId)\n",
    "        decoded_postings_list = []\n",
    "        initialDocId = 0\n",
    "        for docIdsGap in gaps_postings_list:\n",
    "            docIds = initialDocId + docIdsGap\n",
    "            initialDocId = docIds\n",
    "            decoded_postings_list.append(docIds)\n",
    "        return(decoded_postings_list)\n",
    "        ### End your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write test cases for any helper methods implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "pstlst = [12,100,200,245,250,258,275,300]\n",
    "# print(CompressedPostings.encode(pstlst))\n",
    "# print(pstlst)\n",
    "# print(len(pstlst))\n",
    "CompressedPostings.decode(CompressedPostings.encode(pstlst))\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a helper function for testing whether an encoded postings list is decoded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode(l):\n",
    "    e = CompressedPostings.encode(l)\n",
    "#     print(e)\n",
    "    d = CompressedPostings.decode(e)\n",
    "#     print(d)\n",
    "    assert d == l\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few test cases to make sure the postings list compression and decompression is being done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "test_encode_decode(pstlst)\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new folder to store the compressed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('index_dir_compressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir=data_dir, output_dir='index_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance_compressed.retrieve('career fair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done with our search engine implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running time analysis (Optional, NOT graded)\n",
    "\n",
    "In this section you can write queries to evaluate the running time characteristics of `retrieve` and use it to for in-practice understanding of the key optimizations decisions when designing a retrieval algorithm. Note that, due to the very small data collection size, timings may not be perfect (and sometimes results may vary). However, you should still be able to see some timing differences often. Also, note that the following code cells assume there exists an indexed BSBIIndex object.\n",
    "\n",
    "Python offers a convenient timing module `%timeit` ([documentation](https://docs.python.org/3/library/timeit.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit BSBI_instance.retrieve('virginia tech')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you **replace or add** a term in the query so that it runs slower? Time your query to see if indeed is slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "10071\n",
      "2485\n",
      "263\n",
      "50.1 ms ± 2.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# %timeit BSBI_instance.retrieve('virginia tech')\n",
    "### Begin your code\n",
    "%timeit BSBI_instance.retrieve('virginia tech campus')\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why did the retrieve function get slower?**\n",
    "> As the number of terms increased the search engine takes time as it finds the list of document for each of the given terms and then the concatenation of those lists. So this procedure took time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new query by adding a term to the previous query that makes it go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "11539\n",
      "2932\n",
      "30\n",
      "75.7 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "### Begin your code \n",
    "%timeit BSBI_instance.retrieve('virginia tech cs')\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where does the increase in speed come from?** \n",
    "\n",
    "> Decrease in time or increase in speed comes when we have less query term so less time to find postings list and then traverse those list for merging the common term to find the most relevant document but above as it was asked to add some term it didn't decreased the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index size\n",
    "\n",
    "In this section we will look at the size of index on disk.\n",
    "\n",
    "### Uncompressed index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the size of the merged index (`.index`) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of index 3329880 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of index\", os.path.getsize(\"index_dir/BSBI.index\"), 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above queries the file system to compute the size of the index. \n",
    "\n",
    "Show that you can compute the same answer by looking at the `postings_dict` object directly. Your code should compute the size of the index file (in bytes) using the postings dict. You should use the word size (in bytes) and size information stored in the `postings_dict`. The resulting number should match the byte number above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3329880\n"
     ]
    }
   ],
   "source": [
    "with open('index_dir/BSBI.dict', 'rb') as f:\n",
    "    postings_dict, terms = pkl.load(f)\n",
    "    ### Begin your code\n",
    "    totsize = 0\n",
    "    for term in postings_dict:\n",
    "        size = postings_dict[term][2]\n",
    "        totsize = totsize + size\n",
    "    print(totsize)\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Describe how you have computed the expected size based on the information stored in the postings dictionary.**\n",
    "\n",
    "> Postings Dictonary contains the information of every terms postings list. It is basically a tuple of (index file start pointer, number of elements in postings list, length in bytes of the encoded postings list written on the index file). So for a particular element acting as Dictonary Key above tuple is the corresponding value. So basically the file length is the summation of all the length of bytes written in index file. That is how I did summation of all bytes length while traversing dictionary to find the expected size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the size of the compressed index (`.index`) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of compressed index 0 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of compressed index\", os.path.getsize(\"index_dir_compressed/index_0.index\"), 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: How would the size of the compressed index change if variable byte encoding was used on the docIDs instead of the gaps?**\n",
    "\n",
    "> By applying the variable byte encoding on DocIds also would have reduced the size than the original one. So if the corpus has majority of the term having less frequency then applying byte encoding on Docids or gap wouldn't have created a significance differnce but since all corpus generally have stop words (i.e. terms having high frequency) so that helps in reducing the compression size when variable encoding gap is used on gap rather than on Docid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance\n",
    "\n",
    "Let's take a deeper look into improving performance. Think about your answers to the following questions. Note that some questions are subjective and may have multiple correct answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this HW you useD each sub-directory as a block and build an index for one block at a time. Can you discuss the trade-off of different sizes of blocks? Is there a better way to work with limited memory when we want to minimize indexing time?**\n",
    "\n",
    "> There is always a tradeoff between block size and indexing time. If we decrease the block size then the indexing time will pretty much decrease giving us the output in less time that is system has high throughput but less block size means we are cutting off the information of big size blocks which is reducing accuracy thus less efficiency and if we increase the block size then indexing time increases and then throughput is less whereas we are allowing the blocks to have more size this time and have more information to process increasing the accuracy and thus system has high efficiency. Thus there is indirect relation between throughput and efficiency of the system based on block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Is there a part of your indexing program that limits its scalability to larger datasets?**\n",
    "\n",
    "> No, logically not as we can save the dictionary in memory and load it part by part if dataset is larger only thing is that the larger dataset is more time it will take to do every operation be it retriveal or merging the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: Describe other parts of the indexing process that you could optimize for indexing scalability and faster retrieval time.**\n",
    "\n",
    "> Instead of retriving the dictionary and postings list everytime from dictionary we can save it for fast usage but yes that limits to the size of the corpus being used. That is the one change I felt could be done for smaller sized corpus in above implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
